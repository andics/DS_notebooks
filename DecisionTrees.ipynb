{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ne7oShkCmbpi"
   },
   "source": [
    "<img src = \"https://datasciencebocconi.github.io/Images/Other/logoBocconi.png\">\n",
    "\n",
    "$\\newcommand{\\bb}{\\boldsymbol{\\beta}}$\n",
    "$\\newcommand{\\bg}{\\boldsymbol{\\gamma}}$\n",
    "$\\DeclareMathOperator{\\Gau}{\\mathcal{N}}$\n",
    "$\\newcommand{\\bphi}{\\boldsymbol \\phi}$\n",
    "$\\newcommand{\\bx}{\\boldsymbol{x}}$\n",
    "$\\newcommand{\\by}{\\boldsymbol{y}}$\n",
    "$\\newcommand{\\whbb}{\\widehat{\\bb}}$\n",
    "$\\newcommand{\\hf}{\\hat{f}}$\n",
    "$\\newcommand{\\tf}{\\tilde{f}}$\n",
    "$\\newcommand{\\ybar}{\\overline{y}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\Var}{Var}$\n",
    "$\\newcommand{\\Cov}{Cov}$\n",
    "$\\newcommand{\\Cor}{Cor}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zF9BBbFVmbpi"
   },
   "source": [
    "# Training module on non-linear modeling via tree-based methods \n",
    "## Topics covered:\n",
    "+ Decision trees (for regression and classification)\n",
    "+ Bagging\n",
    "+ Random forests\n",
    "+ Boosting and gradient boosting\n",
    "\n",
    "\n",
    "This module introduces some fundamental concepts for effective predictive modelling based on **decision trees** and their extensions relying on esemble of multiple trees (i.e., **bagging**, **random forests** and **boosting**). The models and algorithms discussed here, combined also with linear models and regularisation, provide state-of-the-art solutions often used in practice due to their high prediction accuracy.  \n",
    "\n",
    "As you will see, within this paradigm the features become part of the learning procedure $-$ not merely by selection, as e.g. with lasso.\n",
    "\n",
    "**credits**: some pictures are taken from *The Elements of Statistical Learning* by Hastie, Tibshirani and Friedman (https://hastie.su.domains/ElemStatLearn/), and  *An Introduction to Statistical Learning* by James, Witten, Hastie, Tibshirani (https://www.statlearning.com/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBZnvxbdOKsD"
   },
   "source": [
    "## Summary\n",
    "**Decision trees** for regression and classification rely on partitions of the space of explanatory variables (input space) into a number of simple and non-overlapping **regions**. For each region a summary output value is then generated using data in that region, and such value will be used to predict the response for all those units (present and future) falling in that region.   \n",
    "\n",
    "As we will see through extensive Python implementations, the above approach allows to account for and learn non-linear relations between the output and the inputs in a simple, yet effective, way. This is done through the careful identification of suitable regions via a **sequential binary splitting** strategy which, at each step, produces a new region by splitting in two one of the previously identified regions so that the new resulting partition of the training data has minimum variability, within each region, in terms of the output value to predict. Since the set of splitting rules can be summarized in a tree with branches and leaves, these approaches are known as decision tree methods.\n",
    "\n",
    "The figure below provides an example of a **regression tree**, where we partitioned (in a greedy fashion) the input space in 3 regions and assigned an estimate of the log-salary $y$ to each of them (to be used for prediction).\n",
    "\n",
    "+ $R_1=\\{\\bx : \\texttt{years} < 4.5 \\} \\longrightarrow \\hat{y}_{R_1}=\\mbox{Ave}[y_i : \\bx_i \\in R_1]$\n",
    "+ $R_2=\\{\\bx : \\texttt{years} \\geq 4.5, \\texttt{hits}< 117.5  \\}\\longrightarrow \\hat{y}_{R_2}=\\mbox{Ave}[y_i : \\bx_i \\in R_2]$\n",
    "+ $R_3=\\{\\bx : \\texttt{years} \\geq 4.5, \\texttt{hits}\\geq 117.5  \\}\\longrightarrow \\hat{y}_{R_3}=\\mbox{Ave}[y_i : \\bx_i \\in R_3]$\n",
    "\n",
    "<img src = \"https://datasciencebocconi.github.io/Images/DecisionTrees/example_tree.jpeg\">\n",
    "\n",
    "Focusing again on the above example:\n",
    "+ The regions $R_1, R_2$ and $R_3$ are called **terminal nodes** or **leaves**\n",
    "+ Trees are typically  drawn upside-down (the leaves are at the bottom)\n",
    "+ The points along the tree where the predictor space is split are called **internal nodes**. In the example $\\texttt{years} < 4.5$ and $\\texttt{hits} < 117.5$ are two internal nodes\n",
    "\n",
    "Trees are **explainable**. In the above example the input $\\texttt{years}$  is the most important factor in determining the $\\texttt{log-salary}$, and players with less experience earn lower salaries than more experienced players. Indeed, given that a player is less experienced, the number of $\\texttt{hits}$ made in the previous year seems to play little role on the $\\texttt{log-salary}$. However, among players who have been in the major leagues for five or more $\\texttt{years}$, the number of $\\texttt{hits}$  made in the previous year does affect $\\texttt{log-salary}$.\n",
    "\n",
    "Although basic classification and regression trees (CART) are very interpretable, such solutions can be often improved in terms of predictive accuracy. Therefore, we will also discuss **bagging**, **random forests** and **boosting**, which combine multiple trees and often lead to dramatic improvements in prediction. Their implementations are presented here using sklearn package of Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1645694981484,
     "user": {
      "displayName": "Daniele Durante",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9kxtWoIcGrAkEH2tgp82lyAhf_3EdWvpJRtJmwQ=s64",
      "userId": "13454572102879749218"
     },
     "user_tz": -60
    },
    "id": "haOV0FUjmbpk"
   },
   "outputs": [],
   "source": [
    "# We load the relevant modules\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1645694984211,
     "user": {
      "displayName": "Daniele Durante",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9kxtWoIcGrAkEH2tgp82lyAhf_3EdWvpJRtJmwQ=s64",
      "userId": "13454572102879749218"
     },
     "user_tz": -60
    },
    "id": "z6B9RV9kmbpk"
   },
   "outputs": [],
   "source": [
    "# A helper function to display the tree.\n",
    "# NOTE: requires pydotplus and graphviz libraries. \n",
    "#       for MACOS/LINUX just install by typing \"conda install pydotplus\" and \"conda install -c conda-forge python-graphviz\" at the terminal\n",
    "#       for Windows install by typing \"conda install pydotplus\" at the Anaconda Powershell Prompt \n",
    "#       for Windows and graphviz, if conda install doesn't work, then download the zip file from https://graphviz.gitlab.io/_pages/Download/Download_windows.html\n",
    "#             unzip, include the 'bin' folder path in an environment variable named 'path' then do 'pip install graphviz' and 'conda install graphviz' from anaconda prompt\n",
    "#             finally, also include in the environment variable 'path' the path to the Anaconda graphviz package, i.e. SomePath\\Anaconda3\\Library\\bin\\graphviz\n",
    "    \n",
    "from IPython.display import Image \n",
    "import pydotplus\n",
    "import graphviz\n",
    "def plot_tree(clf, feature_names, target_names):\n",
    "    dot_data = sklearn.tree.export_graphviz(clf, out_file=None, \n",
    "                             feature_names=feature_names,  \n",
    "                             class_names= target_names,  \n",
    "                             filled=True, rounded=True,  \n",
    "                             special_characters=True) \n",
    "    return pydotplus.graph_from_dot_data(dot_data).create_png() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GqMWrJmmbpk"
   },
   "source": [
    "## Regression models with tree-based features\n",
    "\n",
    "To work our way towards tree-based models it is useful to rewrite these constructions as a linear regression model with **tree-based features** \n",
    "\n",
    "Recall again the example below, and let $x_1$ and $x_2$ denote $\\texttt{years}$ and $\\texttt{hits}$, respectively. \n",
    "\n",
    "<img src = \"https://datasciencebocconi.github.io/Images/DecisionTrees/example_tree.jpeg\">\n",
    "\n",
    "Then, the above tree-based model can be expressed as\n",
    "\n",
    "$$f(x_1,x_2) =  5.11 \\cdot 1[(x_1,x_2) \\in R_1] + 6.00 \\cdot 1[(x_1,x_2) \\in R_2] + 6.74\\cdot 1[(x_1,x_2) \\in R_3]$$\n",
    "\n",
    "More generally, if we have $p$ inputs $\\bx=(x_1, \\ldots, x_p)$, then\n",
    "\n",
    "$$f(\\bx;\\bb) =  \\sum_{m=1}^M \\beta_m 1[\\bx \\in R_m] $$\n",
    "\n",
    "where the $R_1, \\ldots, R_M$ are disjoint regions of the input space defined by a binary tree, say splitting each variable on the median. Below, there is a more generic partitioned space of 2 variables.\n",
    "\n",
    "<img src = \"https://datasciencebocconi.github.io/Images/DecisionTrees/example_tree_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "149Bcxqimbpk"
   },
   "source": [
    "As you can notice from the figure above, the assumption here is that the **regression function** $f(\\bx;\\bb)$ is constant in each of these regions, hence the data are split in homogenous subgroups. We are basically fitting a **piecewise constant function**.\n",
    "\n",
    "If the regions were known, it is trivial to fit this regression model to data: Each $\\beta_m$ is estimated from the training that fall in the subregion alone, and it is a simple instance of a linear model. \n",
    "\n",
    "**However**, also the regions $R_1, \\ldots, R_M$ are unknown and need to be \n",
    "estimated. This raises, at least, two main problems with this approach:\n",
    "\n",
    "1. **Computational**: The number of possible regions (and, hence, different partitions) of the input space we should explore is huge. For instance, even in the simple case in which we have $p$ predictors and each is split in its median, this creates $2^p$ regions. Therefore we quickly end up with massive number of features $-$ and will overfit unless we take precautions. \n",
    "2. **Predictive**: In classical Machine Learning settings, we often have a large number $p$ of inputs and it typically turns out that a few, or even most, are not useful for prediction, hence we should not be subdiving the population according to these variables. On the other hand, for those that are relevant for prediction, there is no good reason why the median, or any preselected quantile, provides a good split. We would like to learn from the data itself where to split each variable (if used at all in the model); and maybe in order to get a good predictive model we might have to split some variables several times. Which now feeds back into the \"computational\" problem since the  potential regions might be much larger than $2^p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGDX6Eg7mbpk"
   },
   "source": [
    "### Tree based models: a first illustration\n",
    "\n",
    "Tree-based models, and the associated learning algorithms for fitting them to data, address the two challenges discussed above and result in a non-linear model where the variables to be split and the split points are learned from data (i.e., we learn not only $\\beta_1, \\ldots, \\beta_M$ from the data, but also the regions $R_1, \\ldots, R_M$). \n",
    "\n",
    "Before we discuss more in detail the methods and algorithms to address the above goal, let's immediatelly implement one such algorithm to the **spam** training data. The objective of this particular example is to classify emails as 'spam' or 'no spam', based on features that are basically the occurrence of certain words or expressions.\n",
    "\n",
    "What we are doing below is **bad practice**: never run a function you do not know what it does $-$ especially when it has many options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 225,
     "status": "ok",
     "timestamp": 1645694988982,
     "user": {
      "displayName": "Daniele Durante",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9kxtWoIcGrAkEH2tgp82lyAhf_3EdWvpJRtJmwQ=s64",
      "userId": "13454572102879749218"
     },
     "user_tz": -60
    },
    "id": "QHLxy_0mmbpk"
   },
   "outputs": [],
   "source": [
    "# Load the spam dataset\n",
    "dataset = pd.read_csv(\"https://datasciencebocconi.github.io/Data/spam_small_train.csv\")\n",
    "X = dataset.drop('class', axis = 1)\n",
    "y = dataset[\"class\"]\n",
    "\n",
    "# to print stats\n",
    "feature_names = X.columns\n",
    "class_labels = [\"email\", \"spam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 634,
     "status": "ok",
     "timestamp": 1645694992637,
     "user": {
      "displayName": "Daniele Durante",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9kxtWoIcGrAkEH2tgp82lyAhf_3EdWvpJRtJmwQ=s64",
      "userId": "13454572102879749218"
     },
     "user_tz": -60
    },
    "id": "JBHLvfiymbpk",
    "outputId": "d810bce7-4902-4658-803a-e45b9c98d8ce"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387,
     "output_embedded_package_id": "1RaoPRH9NZbaVQBB9dHIPjqhRk7VpSK8N"
    },
    "executionInfo": {
     "elapsed": 19303,
     "status": "ok",
     "timestamp": 1645695017110,
     "user": {
      "displayName": "Daniele Durante",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9kxtWoIcGrAkEH2tgp82lyAhf_3EdWvpJRtJmwQ=s64",
      "userId": "13454572102879749218"
     },
     "user_tz": -60
    },
    "id": "P1phoVnDmbpm",
    "outputId": "68a0d906-8557-4545-8978-36a7e4a0518f"
   },
   "outputs": [],
   "source": [
    "# plot the tree\n",
    "Image(plot_tree(model, feature_names, class_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5ETxl0cmbpm"
   },
   "source": [
    "Note the **complexity** of this tree. \n",
    "\n",
    "\n",
    "+ Orange colours mean output 'no spam'. \n",
    "+ Blue colours mean 'spam'. \n",
    "\n",
    "The latter are predominant in the right hand side of the tree. You can zoom into the figure by saving (right-clicking) and opening elsewhere.\n",
    "\n",
    "Before we go any further, let's check the performance of the model. In particular, let us compute the **predicted probabilities** for both training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1645695020596,
     "user": {
      "displayName": "Daniele Durante",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9kxtWoIcGrAkEH2tgp82lyAhf_3EdWvpJRtJmwQ=s64",
      "userId": "13454572102879749218"
     },
     "user_tz": -60
    },
    "id": "5xbr5tEhmbpm"
   },
   "outputs": [],
   "source": [
    "# read the test data, extract the info and create predictions\n",
    "\n",
    "spam_test = pd.read_csv(\"https://datasciencebocconi.github.io/Data/spam_small_test.csv\")\n",
    "Xtest = spam_test.drop(\"class\",axis=1)\n",
    "ytest = spam_test[\"class\"]\n",
    "pred = model.predict_proba(X)\n",
    "test_pred = model.predict_proba(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "executionInfo": {
     "elapsed": 452,
     "status": "ok",
     "timestamp": 1645695022194,
     "user": {
      "displayName": "Daniele Durante",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9kxtWoIcGrAkEH2tgp82lyAhf_3EdWvpJRtJmwQ=s64",
      "userId": "13454572102879749218"
     },
     "user_tz": -60
    },
    "id": "CKYfmlltmbpm",
    "outputId": "36ec6444-93bf-4262-d0f1-bc5d7d99c3f8"
   },
   "outputs": [],
   "source": [
    "plt.plot(pred[:,1],\"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1645642403943,
     "user": {
      "displayName": "Daniele Durante",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9kxtWoIcGrAkEH2tgp82lyAhf_3EdWvpJRtJmwQ=s64",
      "userId": "13454572102879749218"
     },
     "user_tz": -60
    },
    "id": "0F5jteMCmbpm",
    "outputId": "37fcc748-a31c-4898-fe44-1dc4f3e53233"
   },
   "outputs": [],
   "source": [
    "plt.plot(test_pred[:,1],\"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExuJ-5Cbmbpm"
   },
   "source": [
    "We observe that the model predicts 'spam' or 'no spam' with apparently high confidence on its results (probabilities close to 0 or 1, few points in between). This is a bit suspicious and possibility caused by **overfitting**. Therefore, it is a good idea to explore more in detail **predictive accuracy**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSm92HLrmbpm"
   },
   "source": [
    "### AUC recap\n",
    "\n",
    "A useful example (defaults on credit cards) for why **ROC** curve is important.\n",
    "\n",
    "|  | True NO | True YES |  Total\n",
    "| --- | --- | --- | ---\n",
    "| **Predicted NO** | 9644 | 252 | 9896\n",
    "| **Predicted YES** | 23 | 81 | 104\n",
    "| **Total** | 9667 | 333 | 10000\n",
    "\n",
    "If we classified using just the prior (always class No in this case) we would make $333/10000 = 3.33\\%$ errors. Moreover, of the true No, we make $23/9667 = 0.2\\%$ errors. Of the true Yes, we make $252/333 = 75.7\\%$.\n",
    "\n",
    "Recall that, for binary classification models, to check performance we use the Area Under the Curve (**AUC**), considering the **ROC** (Receiver Operating Charateristic) curve. \n",
    "\n",
    "Let us study the **AUC for both training and test data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1645642407667,
     "user": {
      "displayName": "Daniele Durante",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9kxtWoIcGrAkEH2tgp82lyAhf_3EdWvpJRtJmwQ=s64",
      "userId": "13454572102879749218"
     },
     "user_tz": -60
    },
    "id": "GASy4UQbmbpm",
    "outputId": "4f3ec749-845c-4d4d-f579-ee4950156dc4"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "fpr, tpr, thresholds = metrics.roc_curve(np.array(y), np.array(pred[:,1]))\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,estimator_name='example estimator')\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1645642409942,
     "user": {
      "displayName": "Daniele Durante",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9kxtWoIcGrAkEH2tgp82lyAhf_3EdWvpJRtJmwQ=s64",
      "userId": "13454572102879749218"
     },
     "user_tz": -60
    },
    "id": "ljWSp9Vqmbpm",
    "outputId": "84e43e39-ad17-4702-b3ef-3df4b7972b83"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "fpr, tpr, thresholds = metrics.roc_curve(np.array(ytest), np.array(test_pred[:,1]))\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "display = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,estimator_name='example estimator')\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oy0dnvxImbpm"
   },
   "source": [
    "## Understanding the model and the algorithm\n",
    "\n",
    "From the Hastie et al. book, here is an example of the **binary tree** representation of the tree-based model and the corresponding **partition of input space** into subregions, as well as the induced **regression function** $f(\\bx;\\bb)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04d7egfsmbpm"
   },
   "source": [
    "<img src = \"https://datasciencebocconi.github.io/Images/DecisionTrees/example_tree_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOn2DQThmbpm"
   },
   "source": [
    "In this construction the model and the algorithms are pretty much interweaved. The loss function can be used pretty much as in our previous supervised learning approaches. However, here it is not entirely straightforward to define an algorithm that tries to minimize the loss function, since in this case we not only need to estimate $\\bb$ but also a potentially complex partition of the input space in regions $R_1, \\ldots, R_M$. More formally, in the regression context under the classical residual sum of squares loss, we look for\n",
    "\n",
    "$$\\mbox{argmin}_{\\beta_1, \\ldots, \\beta_M; R_1, \\ldots, R_M}\\left\\{ \\sum_{i=1}^n\\left(y_i- \\sum_{m=1}^M \\beta_m 1[\\bx_i \\in R_m]\\right)^2\\right\\}$$\n",
    "\n",
    "The above optimization problem is  **non-convex**. Hence, rather than parameterising the regions, one typically uses an algorithm that then dictates the regions. \n",
    "\n",
    "In particular, we can use **recursive binary splitting**: A topâ€“down (begins at the top of the tree and then considers successive binary splits to grow the tree one region at-a-time) greedy (the best split is made at that each step, rather than picking a split that will lead to a better tree in some future step) routine.\n",
    "\n",
    "More specifically, recursive binary splitting uses a heuristic **forward search**, which is somehow reminiscent of the one used for variable selection in regression.\n",
    "+ To grow the tree one region more, all possible variables are considered. For each variable the optimal cutoff point is found and then the variable that achieves largest decrease in loss function is chosen to determine the next level\n",
    "+ This is heuristic but very fast to implement\n",
    "\n",
    "Below you can find an illustrative example of this algorithm.\n",
    "\n",
    "<img src = \"https://datasciencebocconi.github.io/Images/DecisionTrees/binary_splitting_example.png\">\n",
    "\n",
    "This approach is by no means immune to overfitting $-$ and we have seen this in the **spam** example.\n",
    "+ One approach is to bound the depth\n",
    "+ Another is to use a **backward search** after growing the tree at its maximum depth. This is done by pruning back the tree and remove branches in order to obtain a good balance between bias and variance. The backward criterion is effectively a penalized likelihood criterion based again on a **regularization parameter**. \n",
    "+ As usual, a predictive measure is used to estimate the regularizing parameter (depth/backward parameter). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKzL4qMUmbpm"
   },
   "source": [
    "### Behind the scenes\n",
    "\n",
    "+ Non-convex optimisation and heuristics\n",
    "  + forward-backward search\n",
    "  +  model fit criteria for tree-based models (e.g. regression and classification)\n",
    "  +  model complexity criteria for tree-based models; regularisation parameter\n",
    "+ Variable selection and importance\n",
    "+ Categorical predictors and missing data\n",
    "\n",
    "Let's revisit the sklearn class DecisionTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-wrpdJTmbpm"
   },
   "source": [
    "## Complexity, accuracy and overfitting\n",
    "Here we study more in detail the **overfitting** risk in relation to some quantities defining the complexity of the tree. In particular, allowing a growing **number of final leaves** in our tree (i.e., more and more regions), we increase the accuracy on training data, but not necessarily on new (test) data $-$ which would be actually our final goal. \n",
    "\n",
    "By grid-searching the hyperparameter 'max_leaf_nodes', we can find optimum value to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 315
    },
    "executionInfo": {
     "elapsed": 3307,
     "status": "ok",
     "timestamp": 1645642418910,
     "user": {
      "displayName": "Daniele Durante",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9kxtWoIcGrAkEH2tgp82lyAhf_3EdWvpJRtJmwQ=s64",
      "userId": "13454572102879749218"
     },
     "user_tz": -60
    },
    "id": "Hw54_REkmbpm",
    "outputId": "540a6765-3b1e-4b8e-aa97-e7f13591a6d1"
   },
   "outputs": [],
   "source": [
    "# some sklearn tools\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# keep the results in a list\n",
    "complexity_value = []\n",
    "test_accuracy = []\n",
    "train_accuracy = []\n",
    "\n",
    "# loop through possible values of max_leaf_nodes\n",
    "for max_leaf_nodes in range (2, 200): \n",
    "    model  = DecisionTreeClassifier(criterion = \"entropy\", max_leaf_nodes=max_leaf_nodes) \n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # predict both on train and test set\n",
    "    y_pred = model.predict(Xtest)\n",
    "    y_pred_train = model.predict(X)\n",
    "    \n",
    "    # store the data to be used for plotting\n",
    "    train_accuracy.append(accuracy_score(y, y_pred_train))\n",
    "    test_accuracy.append(accuracy_score(ytest, y_pred))\n",
    "    complexity_value.append(max_leaf_nodes)\n",
    "    \n",
    "print (\"Best accuracy: \", max(test_accuracy), 'at max_leaf_nodes = ', complexity_value[np.argmax(test_accuracy)])\n",
    "plt.plot(complexity_value, train_accuracy, label='Train accuracy')\n",
    "plt.plot(complexity_value, test_accuracy, label='Test accuracy')\n",
    "plt.xlabel(\"complexity\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhLKSjonmbpm"
   },
   "source": [
    "## Implicit variable selection and scoring\n",
    "\n",
    "The model and algorithm effectively do a **variable selection** and **scoring**. More specially, we expect that:\n",
    "\n",
    "+ input variables which are **not predictive** drop out (i.e., are never chosen for split)\n",
    "+ input variables which are **predictive** are used to build the tree and, among them, the most important would lead to larger reduction in the loss function when used for a split.\n",
    "\n",
    "We can, therefore, monitor for each variable the **total reduction in the loss function** achived by splits in the tree which use that variable, and then **rank variables** according to this measure. Although, much like for lasso one should not take very literally the variable selection aspect of these algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "executionInfo": {
     "elapsed": 248,
     "status": "ok",
     "timestamp": 1645642424818,
     "user": {
      "displayName": "Daniele Durante",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9kxtWoIcGrAkEH2tgp82lyAhf_3EdWvpJRtJmwQ=s64",
      "userId": "13454572102879749218"
     },
     "user_tz": -60
    },
    "id": "uy1jO_45mbpm",
    "outputId": "a0e44431-58c5-4bb5-a111-75cccc46ad17"
   },
   "outputs": [],
   "source": [
    "model  = DecisionTreeClassifier(max_leaf_nodes=20) \n",
    "model.fit(X, y)\n",
    "important_features = pd.DataFrame(model.feature_importances_/model.feature_importances_.max() ,index=X.columns, columns=['importance'])\n",
    "# it is common to normalize by the importance of the highest\n",
    "important_features.sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "executionInfo": {
     "elapsed": 1726,
     "status": "ok",
     "timestamp": 1645642433012,
     "user": {
      "displayName": "Daniele Durante",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9kxtWoIcGrAkEH2tgp82lyAhf_3EdWvpJRtJmwQ=s64",
      "userId": "13454572102879749218"
     },
     "user_tz": -60
    },
    "id": "APEnY473mbpn",
    "outputId": "15ab8c33-6806-43d8-ae6a-b8d9deac7c94"
   },
   "outputs": [],
   "source": [
    "# Lets see the tree too\n",
    "\n",
    "Image(plot_tree(model, feature_names, class_labels))\n",
    "# do right click save image as and open outside the notebook to see details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7FDCjwhuJj6D"
   },
   "source": [
    "## Some details on classification trees\n",
    "\n",
    "So far, we have presented the details of trees with a main focus on regression (i.e., when $y$ is quantitative). The application to the spam dataset is instead a situation in which $y$ is qualitative (binary in this case). Therefore, before moving to bagging and random forests, it is worth discussing some details on **classification trees**.\n",
    "\n",
    "**Classification trees** are conceptually very similar to regression trees, with the only difference that the variable $y$ of interest is **qualitative**. Hence, we  cannot use the sample mean of $y$ to make predictions and it is not possible to use the MSE as a loss function to perform the splits or to choose among trees.\n",
    "\n",
    "Luckily, the modification required to the regression trees framework are only minor. Compared to regression trees\n",
    "\n",
    "+ $\\hat{y}_{R_m}$ coincides with the  most commonly occurring class of training observations in $R_m$, for each $m=1, \\ldots, M$\n",
    "+ MSE is replaced by  **miss-classification rate**, **Gini index** or **cross-entropy**\n",
    "\n",
    "After these replacements have been made, tree building and pruning remain the same.\n",
    "\n",
    "Note that predicting within each region  ${R_m}, \\ m=1, \\ldots, M$ with the most commonly occurring class is the same as choosing the label having the highest relative frequency (computed from the training observations in $R_m$). In particular, \n",
    "$$\\hat{y}_{R_m}=\\mbox{argmax}_l (\\hat{p}_{ml}), \\quad \\mbox{with} \\quad \\hat{p}_{ml}=\\frac{1}{n_m} \\sum_{i : \\bx_i \\in R_m} 1(y_i=l), \\ l=1, \\ldots, L,$$\n",
    "where $\\hat{p}_{ml}$ is the proportion of class $l$ observations in  region  $R_m$.\n",
    "\n",
    "Based on the above prediction strategy, a natural way to replace the $\\mbox{MSE}_m$ of the generic region ${R_m}, \\ m=1, \\ldots, M$, is to consider the  **miss-classification rate**\n",
    "$$\\mbox{E}_m=\\frac{1}{n_m} \\sum_{i : \\bx_i \\in R_m} 1[y_i \\neq \\mbox{argmax}_l (\\hat{p}_{ml})]=1-\\mbox{max}_l(\\hat{p}_{ml}).$$\n",
    "\n",
    "Using $\\mbox{E}_m$, instead of the $\\mbox{MSE}_m$, for each $m=1, \\ldots, M$, we can easily perform tree building via recursive binary splitting and cost complexity pruning as in regression trees. **However**, it turns out that $\\mbox{E}_m$ is not sufficiently sensitive for tree-growing and, in practice, the **Gini index** or the **cross-entropy** are better.\n",
    "\n",
    "To understand this, consider this **example**: in a 2-class study with $400$ units per class [denoted by $(400, 400)$], suppose that one split created nodes $(300, 100)$ and $(100, 300)$, while the other created nodes $(200, 400)$ and $(200, 0)$. Both splits produce  miss-classification rates of 0.25, but the second provides a pure node and is probably preferable. Both the **Gini index** and the **cross-entropy** are lower for the second split.\n",
    "\n",
    "Both **Gini index** and the **cross-entropy** will take a value close to zero if the region $R_m$ contains mostly observations from a single class, thus providing a **measure of purity**. For these reasons they are preferable when building and pruning trees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CTbA_UHimbpn"
   },
   "source": [
    "## From trees to bagging to random forests\n",
    "\n",
    "\n",
    "What motivates **bagging** and **random forests** is actually an issue of basic decision trees, which affects the predictive performance of these procedures. This issue is the **high variance** (\"instability\") of basic decision trees, which is massively reduced in bagging and random forests via **ensembling**.\n",
    "\n",
    "Therefore, let us first study the stability of trees to better appreciate the subsequent developments we will see in bagging and random forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "executionInfo": {
     "elapsed": 27505,
     "status": "ok",
     "timestamp": 1645642465571,
     "user": {
      "displayName": "Daniele Durante",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9kxtWoIcGrAkEH2tgp82lyAhf_3EdWvpJRtJmwQ=s64",
      "userId": "13454572102879749218"
     },
     "user_tz": -60
    },
    "id": "wpkAHku3mbpn",
    "outputId": "a0cec147-1ebb-4e0d-976a-2f3447db2ef0"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_accuracy_argmax = [] # the maximal test accuracy achieved for each split\n",
    "importance_char = [] # the variable char_! importance\n",
    "\n",
    "for bootsam in np.arange(100):\n",
    "    # split randomly dataset; do not fix the seed to see variation\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "    #\n",
    "    # First search for depth \n",
    "    test_accuracy = []\n",
    "    complexity_value = []\n",
    "    for max_leaf_nodes in np.arange (5, 40): \n",
    "        model  = DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes) \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        test_accuracy.append(accuracy_score(y_test, y_pred))\n",
    "        complexity_value.append(max_leaf_nodes)\n",
    "    test_accuracy_argmax.append(complexity_value[np.argmax(test_accuracy)])\n",
    "    #\n",
    "    # print(\"Optimum max leaf {complexity_value[np.argmax(test_accuracy)]}\")\n",
    "    # Then find and store the relative importance of fare for the chosen tree\n",
    "    \n",
    "    model  = DecisionTreeClassifier(max_leaf_nodes=complexity_value[np.argmax(test_accuracy)]) \n",
    "    model.fit(X_train, y_train)\n",
    "    important_features = pd.DataFrame(model.feature_importances_/model.feature_importances_.max() ,index=X.columns, columns=['importance'])\n",
    "    importance_char.append(important_features.loc[\"char_freq_!\",:].values[0])\n",
    "    \n",
    "# Print the results in a convenient manner\n",
    "result =pd.DataFrame(test_accuracy_argmax,columns=[\"depth\"])\n",
    "result[\"score_char\"] = importance_char\n",
    "result.plot(x=\"depth\",y=\"score_char\",kind=\"scatter\")\n",
    "\n",
    "result.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xztYQf55mbpn"
   },
   "source": [
    "The optimum number of leaves oscillates, so optimum model parameterization can differ depending on the subset of data. \n",
    "\n",
    "**Take home message**: Small variations in the training data yield to large changes in the estimated tree and, as a direct consequence, substatial variability in the predictions. **Trees have high variance**. Bagging and random forests address this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2keusJXJmbpn"
   },
   "source": [
    "## Bagging\n",
    "\n",
    "The earlier experiment in which we have grown a tree-based model on different subsets of the data revealed an important aspect of tree-based models, that of **instability**: small changes in the data (e.g. removing 20% of observations) causes large changes in the learned model. This implies **high variance** in the predictions and, recalling the bias-variance tradeoff, it reduces predictive accuracy for test data.\n",
    "\n",
    "Here is another example with some simulated data from Hastie et al. book\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIi1DKFYmbpn"
   },
   "source": [
    "<img src=\"https://datasciencebocconi.github.io/Images/DecisionTrees/bagging.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQlcNt5tmbpo"
   },
   "source": [
    "**Bootstrap aggregation**, or **bagging**, is a general-purpose procedure for reducing the variance of a statistical learning method. We present it here because it is very useful and frequently used in the context of decision trees (but is more general).\n",
    "\n",
    "**Bagging** (and also random forests as we will see later on) relies on a very simple result from probability.\n",
    "\n",
    "If we have a sample of $B$ variables (e.g., the predictions $\\hat{f}_1(\\bx)=\\hat{f}_1(\\bx;\\hat{\\bb}_1), \\ldots, \\hat{f}_B(\\bx)=\\hat{f}_B(\\bx;\\hat{\\bb}_B)$ provided by $B$ trees trained on different datasets) with variance  $\\sigma^2$ and positive pairwise correlation $\\rho$, then, the average $\\bar{f}(\\bx)$ of these variables has variance $\\sigma^2(1-\\rho)/B+\\rho \\sigma^2$. More formally\n",
    "\n",
    "$$\\mbox{var}(\\bar{f}(\\bx))=\\mbox{var}[(\\sum\\nolimits_{b=1}^B \\hat{f}_b(\\bx))/B]= \\sigma^2(1-\\rho)/B+\\rho \\sigma^2$$\n",
    "\n",
    "In other words, **averaging over a set of observations can reduce variance**. Therefore, it may be possible to reduce the variance by averaging across predictions made by trees estimated on different training sets. \n",
    "\n",
    "+ **Problem**: we have only one training set and not many. \n",
    "+ **Solution**: we can use **bootstrap** to produce, from the single training dataset, several training datasets.\n",
    "\n",
    "Below you can see a graphical representation of the **bootstrap** idea (from the book *An Introduction to Statistical Learning* by James, Witten, Hastie, Tibshirani).\n",
    "\n",
    "<img src=\"https://datasciencebocconi.github.io/Images/DecisionTrees/bootstrap.png\">\n",
    "\n",
    "This is actually what **bagging** does. B(ootstrap) AGG(regation) ING tries to estimate the population-averaged estimator by boostrapping the procedure and averaging across datasets. The resultant learned function is not anymore a decision tree, but a linear combination of trees. This makes bagging an ensemble method $-$ effectively a **model averaging** approach. \n",
    "\n",
    "This is what happens on the same simulated dataset of Hastie et al. book.\n",
    "\n",
    "<img src=\"https://datasciencebocconi.github.io/Images/DecisionTrees/bagged_trees_error.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CUgCueCW7sI"
   },
   "source": [
    "### A graphical illustration of bagging\n",
    "Below you can find a graphical illustration of bagging in the context of **regression**.\n",
    "<img src=\"https://tommasorigon.github.io/DataScienceCourse/Images/DecisionTrees/regression_bag.png\"> \n",
    "\n",
    "As mentioned previously, in general, we do not have multiple training datasets, but we can **bootstrap** from the training data and, for each of them, estimate a tree $T_b$ which outputs a prediction $\\hat{f}_b(\\bx)$ for $y$ at $\\bx$. Clearly, every sample  $b=1, \\ldots, B$ provides a different $\\hat{f}_b(\\bx)$ and, hence, we need to aggregate by averaging, namely\n",
    "$$\\bar{f}(\\bx)=\\sum_{b=1}^B\\hat{f}_{b}(\\bx)/B.$$\n",
    "\n",
    "\n",
    "As illustrated below, bagging of **classification** trees works in a similar manner after minor changes.\n",
    "<img src=\"https://datasciencebocconi.github.io/Images/DecisionTrees/classification_bag.png\"> \n",
    "\n",
    "In the classification setting we generate again $B$ **bootstrap** samples from the training data and, for each of them, we estimate $T_b$ which outputs a predicted class for every $\\bx$ via $\\hat{f}_{b}(\\bx)$. Clearly, every sample $b=1, \\ldots, B$ provides a possibly different predicted class and, hence, we need to aggregate. This is done via **majority vote**: i.e. $\\bar{f}(\\bx)$ provides the most commonly occurring class among the $B$ predictions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6sF6kaIbmbpo"
   },
   "source": [
    "## Random forests\n",
    "\n",
    "To introduce **random forests** recall that if we have a sample of $B$ variables (e.g., the predictions $\\hat{f}_1(\\bx), \\ldots, \\hat{f}_B(\\bx)$ provided by $B$ trees trained on different datasets) with variance  $\\sigma^2$ and positive pairwise correlation $\\rho$, then\n",
    "\n",
    "$$\\mbox{var}(\\bar{f}(\\bx))=\\mbox{var}[(\\sum\\nolimits_{b=1}^B \\hat{f}_b(\\bx))/B]= \\sigma^2(1-\\rho)/B+\\rho \\sigma^2.$$\n",
    "\n",
    "As is clear from the above equation, bagging helps in reducing the first summand, but not the second! In fact, the tree-based estimators from each bootstrap sample are correlated with each other. If they are significantly positively correlated (i.e., $\\rho$ is close to 1), then bagging has little effect and $\\mbox{var}(\\bar{f}(\\bx)) \\approx \\sigma^2$ (same as for a single tree).\n",
    "\n",
    "The above argument motivates a modification of bagging which tries to produce tree-based estimators with as little correlation between them as possible. This is what **random forests** do. \n",
    "\n",
    "More specifically, random forests try to decorrelate the trees grown via bagging from different boostrap samples by including some source of randomness in their growing process (which does not increase the bias too much). Algorithmically, this is done via a minor change to the common forward-backward heuristic optimisation algorithm used for growining trees. At forward steps, instead of considering all variables as candidates for splitting, choose a **random subset** of $k < p$ inputs each time and consider only those. The resulting trees, one for each bootstrap sample, are averaged to produce the random forest estimator. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SETulzaEmbpo"
   },
   "source": [
    "### Bagging and random forests in sklearn\n",
    "\n",
    "Note that random forests are a generalization of bagging which include the latter as a special case (i.e., when $k=p$). Hence, let us focus on implementing **bagging** as a special case of random forests in sklearn considering again the **spam** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1156,
     "status": "ok",
     "timestamp": 1645642476156,
     "user": {
      "displayName": "Daniele Durante",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9kxtWoIcGrAkEH2tgp82lyAhf_3EdWvpJRtJmwQ=s64",
      "userId": "13454572102879749218"
     },
     "user_tz": -60
    },
    "id": "IPQZMbuymbpo",
    "outputId": "7ab9263b-3682-46ea-9c5c-bbccd1b38460"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "np.random.seed(31415) # impose random seed for reproducibility\n",
    "\n",
    "dataset = pd.read_csv(\"https://datasciencebocconi.github.io/Data/spam_small_train.csv\")\n",
    "X = dataset.drop(['class'], axis = 1)\n",
    "y = dataset[\"class\"]\n",
    "\n",
    "# to print stats\n",
    "feature_names = X.columns\n",
    "class_labels = [\"email\", \"spam\"]\n",
    "\n",
    "# Bagging\n",
    "bagging = RandomForestClassifier(n_estimators=20,max_features=11)\n",
    "scores = cross_val_score(bagging, X, y, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peXg8Uo9lUNj"
   },
   "source": [
    "We improve relative to a single tree, but there might be room for additional improvements by including randomness in the tree construction (to decorrelate trees) via the **random forests** idea. Let's see this using also the function *GridSearchCV*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2670,
     "status": "ok",
     "timestamp": 1645642482414,
     "user": {
      "displayName": "Daniele Durante",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9kxtWoIcGrAkEH2tgp82lyAhf_3EdWvpJRtJmwQ=s64",
      "userId": "13454572102879749218"
     },
     "user_tz": -60
    },
    "id": "DHGKr6IKiPIU",
    "outputId": "396fd345-13bb-429a-a373-c0e0dd5bdaa8"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Set the grid of parameters to explore\n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_features': [1,3,5,11],\n",
    "    'n_estimators': [10,20]\n",
    "}\n",
    "\n",
    "# Create a based model\n",
    "rf = RandomForestClassifier(random_state=0) \n",
    "# this is for reproducibility - check documentation!\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 3, n_jobs = -1, verbose = 2)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Check the optimal configuration of parameters\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 575,
     "status": "ok",
     "timestamp": 1645642487174,
     "user": {
      "displayName": "Daniele Durante",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9kxtWoIcGrAkEH2tgp82lyAhf_3EdWvpJRtJmwQ=s64",
      "userId": "13454572102879749218"
     },
     "user_tz": -60
    },
    "id": "JYZfvRyPj2Xk",
    "outputId": "98bdf9e8-edd9-4457-96dd-be4d9b46052a"
   },
   "outputs": [],
   "source": [
    "forest_new = RandomForestClassifier(n_estimators=20,max_features=5)\n",
    "scores = cross_val_score(forest_new, X, y, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "importances = forest_new.fit(X,y).feature_importances_\n",
    "important_features = pd.Series(data=importances/importances.max() ,index=feature_names)\n",
    "important_features.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBwYuKwz6XAg"
   },
   "source": [
    "### Excercise 1\n",
    "As an exercise, check how accuracy is affected when dropping the features that are less important (i.e., importance less than 0.05). Increase the reported decimals if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 236,
     "status": "ok",
     "timestamp": 1645642492349,
     "user": {
      "displayName": "Daniele Durante",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9kxtWoIcGrAkEH2tgp82lyAhf_3EdWvpJRtJmwQ=s64",
      "userId": "13454572102879749218"
     },
     "user_tz": -60
    },
    "id": "ZRD2-46Z6f27",
    "outputId": "d97acc39-7048-4f74-e302-5e23f01096cc"
   },
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 418,
     "status": "ok",
     "timestamp": 1645642494873,
     "user": {
      "displayName": "Daniele Durante",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9kxtWoIcGrAkEH2tgp82lyAhf_3EdWvpJRtJmwQ=s64",
      "userId": "13454572102879749218"
     },
     "user_tz": -60
    },
    "id": "ePyrzE9T7Fac",
    "outputId": "f122349a-ed51-4a00-b7b1-fa53098b4711"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAXsMoo1eu2-"
   },
   "source": [
    "**Some comments** before moving to boosting.\n",
    "\n",
    "+ Variable importance measures can be obtained also in random forests and bagging\n",
    "+ There are 2 main tuning parameters in random forests: $k$ and $B$. In general $k$ is  fixed at some value close to $\\sqrt{p}$ or is selected via cross-validation. Regarding $B$, it can be shown that the global error converges to a lower bound when $B$ grows. Hence, $B$ is typically fixed at a conservative value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NTlmO_1mmbpo"
   },
   "source": [
    "## Boosting \n",
    "\n",
    "Also **boosting** combines multiple trees. However,  unlike bagging and random forests, these trees are grown sequentially using weighted versions of the training data, with the weights depending on the errors made by the previous trees. **Boosting learns from its errors!**. \n",
    "\n",
    "Boosting is general (it can be applied to many ML methods), but here we focus on regression and classification trees, since combining boosting with tree structures often yield the best results in practice.\n",
    " \n",
    "Unlike fitting a single large decision tree (which amounts to  fitting the data hard and potentially overfitting) the boosting approach instead **learns slowly from its errors**. \n",
    "\n",
    "+ Learning from errors has a main impact in terms of **bias reduction**. \n",
    "+ Doing it slowly, allows to **avoid rapid overfitting** and, hence, excessive variance.\n",
    "\n",
    "Boosting has links to a number of **key ideas** in machine learning:\n",
    "\n",
    "+ Bagging, by doing various passes through the data and combining the resulting estimators.\n",
    "+ Forward search for heuristic optimisation, as it turns out that a popular implementation does exactly that.\n",
    "+ Linear models with regularisation, as it turns out it is a linear combination of features, partially linear in the parameters, with a regulariser for complexity. \n",
    "\n",
    "Boosting combines all the above general ideas.\n",
    "\n",
    "There are many ways to arrive to boosting algorithms and to some extent it is a matter of taste which seems more intuitive. Here we follow the derivation of boosting via forward stegewise additive modelling, along the lines in the book by Hastie, T., Tibshirani, R., Friedman, J., 2009. Elements of Statistical Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting and forward stegewise additive modelling\n",
    "\n",
    "Consider a so-called **weak learner**, which is a low bias-high variance predictive model, $b(\\bx,\\bg_b)$. From those we will build an **additive meta-learner** \n",
    "\n",
    "$$f_B(\\bx) = \\sum_{b=1}^B \\beta_b b(\\bx,\\gamma_b) \n",
    "             = f_{M-1}(\\bx) + \\beta_M b(\\bx,\\gamma_B)$$\n",
    "             \n",
    "by learning in a **forward-search** way the parameters $\\beta_b,\\gamma_b$.              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are some standard cases:\n",
    "\n",
    "\n",
    "\n",
    "1. Gaussian regression with general weak learner\n",
    "\n",
    "    The loss for data point $i$ becomes \n",
    "\n",
    "    $$( y_i - f_B(\\bx_i) )^2  = (r_i - \\beta_B b(\\bx,\\gamma_B))^2$$\n",
    "\n",
    "    for $r_i = y_i - f_{B-1} (\\bx_i)$ the residual, hence fit the weak learner on the last residuals. For every model for which fitting the weak learner on the data is easy, you can trivially apply boosting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Classification with weak learners in {-1,+1} and exponential loss - **AdaBoost**\n",
    "\n",
    "      weak learner $b(\\bx,\\gamma_B)$ should return ${+1,-1}$, y should be binary (coded also +1,-1) and the loss for data point i should be:\n",
    "\n",
    "    $$\\exp(- y_i f_B(\\bx_i)) = w_i \\exp(-y_i \\beta_B b(\\bx,\\gamma_B))$$ \n",
    "    \n",
    "    for $w_i = \\exp(-y_i f_{B-1}(\\bx_i))$, hence we obtain, relative to fitting the weak learner only, a weighted loss function at each step of the boosting algorithm.\n",
    "    \n",
    "    Clever but simple manipulation yields the original AdaBoost algorithm which was derived in a different way \n",
    "      - see p.338 - 339 of Elements of Statistical Learning\n",
    " \n",
    "    This loss function is closely related to **logistic link**\n",
    " \n",
    "    An example of such a weak learner: a tree with 2 leaves and +1,-1 output (instead of local means) - a **stump**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Gradient boosting\n",
    "\n",
    "    For squared loss function learning the additive weak learner expansion is easy as we saw earlier. \n",
    "\n",
    "    For other loss functions we can take advantage of this simplicity by computing the negative gradient of the loss function for each data point with respect to the function we wish to learn, evaluated at the current estimate\n",
    "\n",
    "    $$- d L(y_, f(\\bx_i))/d f(\\bx_i) |_{f=f_{B-1}}$$\n",
    "\n",
    "    When $L(y,f) = (y-f)^2$ this is exactly the regression residual $y-f_B$. For others (e.g. logistic link) it will be something different. The idea then is to fit a tree regression model (weak learner) on these \"residuals\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://datasciencebocconi.github.io/Images/DecisionTrees/gdb.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other extensions\n",
    "\n",
    "+ **Xgboost**: Extreme Gradient Boosting (**XGBoost**): Similar to gradient boosting but each iteration solves the gradient search in one single step using Taylor expansion of the gradient formula. On the other hand, it uses a more regularized model formalization to control over-fitting (l1, l2 parameters), which typically implies better performance.\n",
    "\n",
    "#### Caveats\n",
    "The risk of overfitting on those models has to be treated by using some **regularization parameters** (e.g. to limit the number of leaves) and/or using cross validation.\n",
    "\n",
    "In particular, there is an issue called over-specialization, which means that trees added at later iterations tend to impact the prediction of only a few observations, so that their contribution towards the rest of the dataset becomes negligible. This is only addressed by Extreme Gradient Boosting model, by using regularization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-5JafSkmbpo"
   },
   "source": [
    "Lets see **boosting** in action for the **spam dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2152,
     "status": "ok",
     "timestamp": 1645642508224,
     "user": {
      "displayName": "Daniele Durante",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9kxtWoIcGrAkEH2tgp82lyAhf_3EdWvpJRtJmwQ=s64",
      "userId": "13454572102879749218"
     },
     "user_tz": -60
    },
    "id": "8ut5aPZqmbpo",
    "outputId": "c8b0e6b8-d390-406c-b48c-5b72c6ffc6e7"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "np.random.seed(3123) # impose random seed for reproducibility\n",
    "\n",
    "tree = GradientBoostingClassifier(n_estimators=50)\n",
    "scores = cross_val_score(tree, X, y, cv=10,  scoring='roc_auc')\n",
    "print(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "importance = tree.fit(X,y).feature_importances_\n",
    "important_features = pd.Series(data=importance/importance.max() ,index=feature_names)\n",
    "important_features.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LPl9Noc66nRQ"
   },
   "source": [
    "### Exercise 2\n",
    "As an exercise, check how accuracy is affected when dropping the features that are less important (i.e. importance less than 0.05). Increase the reported decimals if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1020,
     "status": "ok",
     "timestamp": 1645642514758,
     "user": {
      "displayName": "Daniele Durante",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gi9kxtWoIcGrAkEH2tgp82lyAhf_3EdWvpJRtJmwQ=s64",
      "userId": "13454572102879749218"
     },
     "user_tz": -60
    },
    "id": "q6B27f_G6slN",
    "outputId": "269f0da3-d8d8-435b-b7f2-ddcbf06fd997"
   },
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoZY9Znfmbpo"
   },
   "source": [
    "## Some hints for practitioners\n",
    "\n",
    "### Models\n",
    "#### CART (single tree)\n",
    "Use *sklearn* function $DecisionTreeClassifier$ for classification, or $DecisionTreeRegressor$ for regression. These models are simpler to understand and explore but typically yield worse results than the ones based on 'multiple tree' approach. Some interesting parameters  to tune (for classification) are:\n",
    "+ *max_depth*: Number of levels in a tree\n",
    "+ *min_samples_split*: Minimum number of samples left to try a new split\n",
    "+ *min_samples_leaf*: Minimum number of samples allowed in a leaf\n",
    "+ *min_impurity_decrease*: Don't accept the split if we don't reach a minimum decrease in impurity\n",
    "+ *min_impurity_split*: Early stop criteria, don't split if mininum impurity has been reached\n",
    "+ *class_weight*: Use 'balanced' if classes are unbalanced.\n",
    "\n",
    "\n",
    "#### Random forest\n",
    "Use *sklearn* function $RandomForestClassifier$ for classification, or $RandomForestRegressor$ for regression. Main parameters are:\n",
    "+ *n_estimators*: Number of trees to build\n",
    "+ *max_features*: Number of features to consider when looking for the best split. If less than 100%, this intruduces stochasticity.\n",
    "+ *boostrap*: Keep default 'True' to allow boostrapping the sample for each tree, useful to prevent overfitting\n",
    "+ *oob_score*: If True, yields a proxy for expected accuracy, using *out of bag* observations per tree.\n",
    "+ *n_jobs*: Number of CPU cores to be used (parallelization).\n",
    "+ *max_depth*, *min_samples_split*, *min_impurity_decrease*, *min_impurity_split*, *class_weight*: Similar to CART\n",
    "\n",
    "\n",
    "#### AdaBoost\n",
    "Use *sklearn* function $AdaBoostClassifier$ for classification, or $AdaBoostRegressor$ for regression. Main parameters are:\n",
    "+ *n_estimators*: Number of trees to build\n",
    "+ *learning_rate*: Intensity of the re-weighting (boosting) each time we build a new tree.\n",
    "+ *base_estimator*: Weak learner, by default 'DecisionTreeClassifier(max_depth=1)', but one can for instance increase depth to check if this helps.\n",
    "\n",
    "#### GradientBoosting (GBM)\n",
    "Use *sklearn* function $GradientBoostingClassifier$ for classification, or $GradientBoostingRegressor$ for regression. Main parameters are:\n",
    "+ *n_estimators*: Number of trees to build\n",
    "+ *subsample*: Percentage of samples to use for every tree.\n",
    "+ *learning_rate*: similar to AdaBoost\n",
    "+ *max_features*,*max_depth*, *min_samples_split*, *min_impurity_decrease*, *min_impurity_split*, *class_weight*: Similar to random forest\n",
    "\n",
    "#### Extreme Gradient Boosting (XGBoost)\n",
    "Use *xgboost* package and follow documentation at https://xgboost.readthedocs.io/en/latest/python/\n",
    "\n",
    "Main functions are $xgboost.XGBClassifier$ or $xgboost.XGBRegressor$, which are an API to be used with *sklearn*.\n",
    "\n",
    "Main parameters are:\n",
    "+ *gamma*: similar to min_impurity_decrease. Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "+ *colsample_bytree*:  Percentage of features to consider when constructing each tree.\n",
    "+ *reg_alpha*:  L1 regularization term on weights to control overfitting\n",
    "+ *reg_lambda*: L2 regularization term on weights to control overtiffing\n",
    "+ *n_estimators*,*subsample*, *learning_rate*,*max_depth*: Similar to Gradient Boosting. \n",
    "\n",
    "#### Other MART (Multiple Additive Regression Trees) models:\n",
    "+ *ligthgbm* package: Computationally faster than GBM, https://lightgbm.readthedocs.io/en/latest/Python-Intro.html\n",
    "+ *catboost* package: Computationally faster than GBM, able to manage categorical variables, allows using GPU, https://github.com/catboost/catboost and https://catboost.ai/\n",
    "+ Extremely Randomized Trees: Similar to random forest but optimization of splits is based on discrete random thresholds instead of exploring the entire space, so it's faster (and sometimes more accurate since is less prone to overfit). Use *sklearn* function $ExtraTreesClassifier$ or *ExtraTreesRegressor*\n",
    "\n",
    "### Hyperparameter optimization\n",
    "Use *sklearn* functions such as $GridSearchCV$ or $RandomizedSearchCV$. See example above in random forest section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUj9oKCsmbpo"
   },
   "source": [
    "## References\n",
    "\n",
    "Friedmann, J., 2001. *Greedy Function Approximation: A Gradient Boosting Machine*. The Annals of Statistics 29(5) https://statweb.stanford.edu/~jhf/ftp/trebst.pdf\n",
    "\n",
    "Hastie, T., Tibshirani, R., Friedman, J., 2009. *Elements of Statistical Learning*. 2nd Edition. Chapters 4.5, 9.1, 9.2, 10 and 15.  https://web.stanford.edu/~hastie/ElemStatLearn/\n",
    "\n",
    "Chen, T., Guestrin, C., 2016. *XGBoost: A Scalable Tree Boosting System*. Proceedings at Conference of Knowledge Discovery and Data Mining, August 13-17, 2016, San Francisco, CA, USA. https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf\n",
    "\n",
    "XGBoost tutorials. https://xgboost.readthedocs.io/en/latest/tutorials/index.html"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DecisionTrees_tutorcopy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "name": "DecisionTrees.ipynb",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "548.867px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": null,
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
