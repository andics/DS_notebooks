{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwivRPaGbN7k"
   },
   "source": [
    "<img src = \"https://datasciencebocconi.github.io/Images/Other/logoBocconi.png\">\n",
    "$\\newcommand{\\bb}{\\boldsymbol{\\beta}}$\n",
    "$\\DeclareMathOperator{\\Gau}{\\mathcal{N}}$\n",
    "$\\newcommand{\\bphi}{\\boldsymbol \\phi}$\n",
    "$\\newcommand{\\bx}{\\boldsymbol{x}}$\n",
    "$\\newcommand{\\bu}{\\boldsymbol{u}}$\n",
    "$\\newcommand{\\by}{\\boldsymbol{y}}$\n",
    "$\\newcommand{\\whbb}{\\widehat{\\bb}}$\n",
    "$\\newcommand{\\hf}{\\hat{f}}$\n",
    "$\\newcommand{\\tf}{\\tilde{f}}$\n",
    "$\\newcommand{\\ybar}{\\overline{y}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\Var}{Var}$\n",
    "$\\newcommand{\\Cov}{Cov}$\n",
    "$\\newcommand{\\Cor}{Cor}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rs4P4RiXbN7k"
   },
   "source": [
    "# High-dimensional predictive regression models\n",
    "\n",
    "We start implementing the first models/algorithms for prediction, and understand to some depth their methodological underpinning \n",
    "\n",
    "\n",
    "For the implementations we will rely on another Python toolkit\n",
    "\n",
    "\n",
    "http://scikit-learn.org/\n",
    "\n",
    "A python library that provides a variety of tools  for machine learning (e.g., pre-process data, evaluate models, etc), and implements a lot of ML algorithms \n",
    "\n",
    "More information can be found here\n",
    " http://scikit-learn.org/stable/documentation.html\n",
    " \n",
    " *Warning: pandas are great for data management and sklearn is useful for ML but they are not (yet) perfectly compatible. sklearn works with numpy objects*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkPwgw3zbN7l"
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this module we build linear models for regression as linear combination of features extracted from original input data. Key ideas we develop here is how to learn the model and how to evaluate model performance. We discuss the bias-variance tradeoff, and related notions such as stability, overfitting, regularization. We introduce a nice algorithmic framework for predictive modelling with a large number of features, that of penalized likelihood generally and the lasso specifically. We also consider model selection questions and methods, based on cross-validation, bootstrap and information criteria. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 930,
     "status": "ok",
     "timestamp": 1644881531587,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "YWt5WHD4bN7l"
   },
   "outputs": [],
   "source": [
    "# We load the relevant modules\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import pandas as pd\n",
    "#seaborn is a module for figures\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VQfTxG0bN7m"
   },
   "source": [
    "## Read the curve data\n",
    "\n",
    "These are our *training data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 334,
     "status": "ok",
     "timestamp": 1644881534310,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "TIxeBszCbN7m"
   },
   "outputs": [],
   "source": [
    "cdata = pd.read_csv('https://datasciencebocconi.github.io/Data/curve_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1644881535797,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "_-Wse1DxbN7m",
    "outputId": "c61c9b5f-25b5-408b-d529-da153127f412"
   },
   "outputs": [],
   "source": [
    "cdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "executionInfo": {
     "elapsed": 802,
     "status": "ok",
     "timestamp": 1644881538976,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "KsTmhNyubN7m",
    "outputId": "7417347c-5f73-445b-b3a2-c5d07ee0c4d2"
   },
   "outputs": [],
   "source": [
    "# plot here is a DF method\n",
    "cdata.plot(x='x',y='y',kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wtqurnh0bN7n"
   },
   "source": [
    "## Our first learning function: linear in $x$\n",
    "\n",
    "$$ y_i \\sim \\Gau(f(x_i,\\bb) , v)$$\n",
    "\n",
    "$$ f(x,\\mathbf{\\beta}) = \\beta_0 + \\beta_1 x$$\n",
    "\n",
    "$$ \\bb = (\\beta_0,\\beta_1)^T$$\n",
    "\n",
    "Remark on the notation: bold-face for vectors, otherwise scalars; bold-face capital letters for matrices\n",
    "\n",
    "We first import the relevant tools. For predictive modelling, which is the aim here, `LinearRegression` is good enough. I would not use this for inference though. `statsmodels.api` appears a better choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 319,
     "status": "ok",
     "timestamp": 1644881542642,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "baewa9y3bN7o"
   },
   "outputs": [],
   "source": [
    "# Importing the relevant sklearn tools\n",
    "# CHECK THE DOC!! \n",
    "\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1644881545251,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "-wWyw9g6bN7o"
   },
   "outputs": [],
   "source": [
    "# Notice the class structure\n",
    "regr  = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 346,
     "status": "ok",
     "timestamp": 1644881546647,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "3Ng8lnCqbN7o"
   },
   "outputs": [],
   "source": [
    "# preparing the data for sklearn - you can appreciate the earlier \n",
    "# \"not very compatible\" comment\n",
    "# we create an array with data provided by the DF\n",
    "data = np.array(cdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1644881548033,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "iq_NmEuMbN7o"
   },
   "outputs": [],
   "source": [
    "# create now predictors and response - and make sure they are \n",
    "# in the right format - they are not by default hence the reshape\n",
    "X = data[:,0].reshape(10,1)\n",
    "y = data[:,1].reshape(10,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 260,
     "status": "ok",
     "timestamp": 1644881549394,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "vOAdkKUwbN7o",
    "outputId": "5a08f3df-8f9c-4c4b-9736-3300511f08d6"
   },
   "outputs": [],
   "source": [
    "# I do not have to define a column of 1s since intercept can be added\n",
    "# in the options\n",
    "# notice the application of the method to the regr instance\n",
    "regr.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdQa_wDBbN7o"
   },
   "source": [
    "### Predicting new data\n",
    "\n",
    "We will compute the *learning function* $f(x,\\bb)$ on some *test data* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 852,
     "status": "ok",
     "timestamp": 1644881552201,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "xfJV9uwWbN7o",
    "outputId": "552856eb-f22c-49d0-bd3b-ab24521f2983"
   },
   "outputs": [],
   "source": [
    "# choose some points to predict - notice reshape and -1! \n",
    "X_new = 0.01*np.arange(100).reshape(-1, 1)\n",
    "#the learned function f(x) at prediction inputs\n",
    "f_new_pred = regr.predict(X_new) \n",
    "# Plot the test data\n",
    "plt.figure()\n",
    "# plot training data\n",
    "plt.scatter(X, y, c=\"orange\", label=\"training data\", alpha=0.5)\n",
    "# plot predictions \n",
    "plt.plot(X_new, f_new_pred, c=\"red\", label=\"test data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fA0gzzB0bN7o"
   },
   "source": [
    "## A better learning function: linear in parameters and features, non-linear in input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7r6EAv0BbN7o"
   },
   "source": [
    "\\begin{equation}\n",
    "y_i \\sim \\Gau(f(x_i,\\bb) , v)\n",
    "\\end{equation}\n",
    "\n",
    "$$ f(x,\\bb) = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\ldots \\beta_n x^{p-1} $$\n",
    "\n",
    "The following is a constructive perspective: we create **features**, that is new input variables that are transformations of the original ones. In the above construction the vector of features for the $i$th data point are \n",
    "\n",
    "$$\\bphi_i =(1,x_i,x_i^2,\\ldots,x_i^{p-1})^T$$ \n",
    "\n",
    "then \n",
    "\n",
    "$$ f(x_i,\\bb) = \\bb^T \\bphi_i$$\n",
    "\n",
    "Notice that we now have $p$ predictors, even though $x$ is 1-dimensional. The choice of polynomial features is simply for illustration; in fact this is not such a good choice for a number of reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1644881555507,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "rn9Y44YkbN7o",
    "outputId": "27314052-beda-48c7-92ad-5fc7769f9016"
   },
   "outputs": [],
   "source": [
    "# Getting more features for the given input. \n",
    "# Polynomial features are so common that sklearn has a built in function\n",
    "# for constructing them \n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures as plf\n",
    "# the argument specifies the polynomial order, here we choose up to power 3\n",
    "poly = plf(3)\n",
    "F = poly.fit_transform(X) #F for fearure matrix\n",
    "print(F) # notice that the intercept is now added by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 235,
     "status": "ok",
     "timestamp": 1644881557981,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "FwlmMwUKbN7p",
    "outputId": "bd482524-8da6-4a56-d233-cb948a5cdc5a"
   },
   "outputs": [],
   "source": [
    "regr  = LinearRegression(fit_intercept=False)\n",
    "regr.fit(F,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "executionInfo": {
     "elapsed": 727,
     "status": "ok",
     "timestamp": 1644881560575,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "YZBcTPRPbN7p",
    "outputId": "d84f672a-9911-442e-99c1-c5332be56587"
   },
   "outputs": [],
   "source": [
    "# predict with the new model: note that we need to generate features\n",
    "# for the test data\n",
    "X_new = 0.01*np.arange(100).reshape(-1, 1)\n",
    "F_new = plf(3).fit_transform(X_new)\n",
    "f_new_pred = regr.predict(F_new) \n",
    "plt.figure()\n",
    "plt.scatter(X, y, c=\"orange\", label=\"training data\", alpha=0.5)\n",
    "plt.plot(X_new, f_new_pred, c=\"red\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyM3uQBabN7p"
   },
   "source": [
    "## Behind the scenes: the learning function, convexity and algorithm\n",
    "\n",
    "In more generality: output $y_i$ (1-d), input $\\bx_i$ (multi-d), features vector for each individual $i$, $\\bphi_i$, a linear model for the learning function $f(\\bx_i,\\bb) = \\bphi_i^T \\bb$\n",
    "\n",
    "The first part of the model $y_i \\sim \\Gau(f(\\bx_i,\\bb) , v)$ quantifies the datapoint-model match. Every data point **scores** the model by how predictable it is by the model, i.e., according to the density $p(y_i | \\bx_i)$; the higher the density the better the model predicts the data point. \n",
    "\n",
    "It is more convenient to work in a different scale: \n",
    "\n",
    "$$-\\log p(y_i \\mid \\bx_i)$$\n",
    "\n",
    "Now this a **datapoint-model mismatch**, the lower this is the better. \n",
    "The overall **data-model mismatch** is obtained by aggregating the evidence by all data. Assuming that the data are independent we have that \n",
    "\n",
    "$$p(y_1,\\ldots,y_n \\mid \\bx_1,\\ldots,\\bx_n) = p(y_1 \\mid \\bx_1) \\cdots p(y_n\\mid \\bx_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvHEmwV3bN7p"
   },
   "source": [
    "Having also assumed Gaussian errors, \n",
    "\n",
    "$$-\\log p(y_i | \\bx_i) = {1 \\over 2 v}(y_i - f(\\bx_i,\\bb))^2 - {1 \\over 2} \\log v - {1\\over 2} \\log(2 \\pi)$$\n",
    "\n",
    "from which we obtain the **loss function**, here  the **negative log-likelihood**\n",
    "\n",
    "$$L(\\bb,v) =  {1 \\over 2 v} \\sum_{i=1}^n (y_i - f(\\bx_i,\\bb))^2 - {n \\over 2} \\log v$$\n",
    "\n",
    "This is a nice function to optimize. First, note that one can optimize over $\\bb$ regardless of the value of $v$: \n",
    "\n",
    "\n",
    "$$\n",
    "\\whbb = \\arg \\min_\\bb L(\\bb,v)\n",
    "$$\n",
    "\n",
    "Once $\\whbb$ has been obtained, we can easily optimize over $v$.  $L(\\bb,v)$ as a function of $\\bb$ is **convex**, in fact it is a quadratic form and one way to optimize is to reduce the computation to a solution of a linear system  \n",
    "\n",
    "What `LinearRegression.fit` does is solve the least-squares minimization problem. \n",
    "Then, the learning function is estimated by \n",
    "\n",
    "$$\\hf(\\bx) = f(\\bx,\\whbb)$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwsLf7GzbN7p"
   },
   "source": [
    "## Model evaluation\n",
    "\n",
    "Lets plot the training $y_i$ vs $\\hf(\\bx_i)$. We will also report the **square of the correlation coefficient** between the two samples. This squared correlation coefficient is used so frequently that has a name: coefficient R-squared\n",
    "\n",
    "A small calculation shows that \n",
    "\n",
    "$$R-squared = 1 - {\\sum_i (y_i - \\hf(\\bx_i))^2 \\over \\sum_i (y_i - \\ybar)^2}$$\n",
    "\n",
    "hence large $R-squared$ is equivalent to small **sum of squared errors**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "executionInfo": {
     "elapsed": 370,
     "status": "ok",
     "timestamp": 1644881565241,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "rAZSu7jHbN7p",
    "outputId": "81e0a2a6-4b42-40ad-fd94-85c34e628461"
   },
   "outputs": [],
   "source": [
    "# plot predicted vs observed - y_hat = f(x)\n",
    "\n",
    "y_hat = regr.predict(F)\n",
    "plt.figure()\n",
    "plt.scatter(x=y,y=y_hat) \n",
    "plt.plot(y,y,c=\"red\")\n",
    "rho = pd.Series(y[:,0]).corr(pd.Series(y_hat[:,0]))\n",
    "plt.title('R-squared equals %.3f' %rho**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzxiISnzbN7p"
   },
   "source": [
    "The big question is - statistically speaking - what do these sample statistics, such as R-squared, estimate and whether they are doing a good job at estimating it \n",
    "\n",
    "The following are some *population* quantities worth estimating. Let $(x^*,y^*)$ be a randomly chosen *test* datapoint from the same phenomenon that has generated the *training data* $(x_i,y_i),i=1,\\ldots,n$. What would be interesting to compute - if possible - is \n",
    "\n",
    "+ Mean Squared Error (MSE):\n",
    "\n",
    "$$ MSE = \\E[(y^* - \\hf(\\bx^*))^2]$$ \n",
    "\n",
    "+ Squared correlation between $y^*$ and $\\hf(\\bx^*)$:\n",
    "\n",
    "$$ R^2 = \\Cor(y^*,\\hf(\\bx^*))^2$$\n",
    "\n",
    "Replacing population expectation with sample averages, and using the training data as samples, we get the quantities we introduced earlier. Therefore, those are statistical estimators of the population quantities above. Are they any good though? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "du8qt53bbN7p"
   },
   "source": [
    "We will return to this question but for the time being lets consider an alternative estimator of these quantities, the so-called **leave-one-out cross-validation estimator**. Intuitively it is simple: \n",
    "\n",
    "+ Use all data points but the $i$th to estimate the learning function: \n",
    "\n",
    "$$\n",
    "\\hat{f}^{-i}(\\bx)\n",
    "$$\n",
    "\n",
    "+ Using the estimated learning function, predict the $i$th training data point: \n",
    "\n",
    "$$\\hf^{-i}(\\bx_i)$$\n",
    "\n",
    "+ Estimate the MSE or $R^2$ by computing\n",
    "\n",
    "$$(y_i - \\hf^{-i}(\\bx_i))^2$$\n",
    "\n",
    "+ We can actually do this for each data point $i$ and then average the estimates: \n",
    "\n",
    "$${1 \\over n} \\sum_{i=1}^n (y_i - \\hf^{-i}(\\bx_i))^2$$ \n",
    "\n",
    "We can implement this ourselves - or use some `sklearn` functions too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "executionInfo": {
     "elapsed": 686,
     "status": "ok",
     "timestamp": 1644881569585,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "L4VJf0sabN7p",
    "outputId": "00f43137-31b4-4328-d839-099eaa6928e5"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict as cvp\n",
    "y_hat_cv = cvp(regr, F, y, cv=10) # this is leave-one-out CV when cv=10      \n",
    "                                  # and 10 because n=10\n",
    "plt.figure()\n",
    "plt.scatter(x=y,y=y_hat_cv) \n",
    "plt.plot(y,y,c=\"red\")\n",
    "rho = pd.Series(y[:,0]).corr(pd.Series(y_hat_cv[:,0]))\n",
    "plt.title('Leave-one-out CV R-squared equals %.3f' %rho**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcvlHFP3bN7q"
   },
   "source": [
    "### Working with a more flexible model\n",
    "\n",
    "We repeat this analysis but working with polynomial of order 7; in this case the number of parameters is almost the same as number of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "executionInfo": {
     "elapsed": 573,
     "status": "ok",
     "timestamp": 1644881572411,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "V0CvcGV6bN7q",
    "outputId": "5b7f77a6-83bb-4200-8a29-518a20b59c38"
   },
   "outputs": [],
   "source": [
    "order = 7\n",
    "poly = plf(order)\n",
    "Flarge = poly.fit_transform(X)\n",
    "regr.fit(Flarge,y)\n",
    "\n",
    "X_new_= 0.01*np.arange(100).reshape(-1, 1)\n",
    "F_new = plf(order).fit_transform(X_new)\n",
    "f_new_pred = regr.predict(F_new) \n",
    "plt.figure()\n",
    "plt.scatter(X, y, c=\"orange\", label=\"training data\", alpha=0.5)\n",
    "plt.plot(X_new, f_new_pred, c=\"red\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcxCJsRhbN7q"
   },
   "source": [
    "Visually the estimated learning function looks good and very similar to the one we obtained with the 3rd order polynomial. \n",
    "\n",
    "Visual inspection is **crucial** - but often not an option. Lets look at our model evaluation criteria too - these are pretty much always available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "executionInfo": {
     "elapsed": 759,
     "status": "ok",
     "timestamp": 1644881575210,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "dwo0TlfVbN7q",
    "outputId": "7202709c-27a2-43b8-da2a-e8e6e325644f"
   },
   "outputs": [],
   "source": [
    "y_hat = regr.predict(Flarge)\n",
    "plt.figure()\n",
    "plt.scatter(x=y,y=y_hat) \n",
    "plt.plot(y,y,c=\"red\")\n",
    "rho = pd.Series(y[:,0]).corr(pd.Series(y_hat[:,0]))\n",
    "plt.title('R-squared equals %.3f' % rho**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "executionInfo": {
     "elapsed": 741,
     "status": "ok",
     "timestamp": 1644881577802,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "IVlj0fPLbN7q",
    "outputId": "851dfe7c-47e6-40fc-d8dd-4a7276506eaa"
   },
   "outputs": [],
   "source": [
    "y_hat_cv = cvp(regr, Flarge, y, cv=10)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(x=y,y=y_hat_cv) \n",
    "plt.plot(y,y,c=\"red\")\n",
    "rho = pd.Series(y[:,0]).corr(pd.Series(y_hat_cv[:,0]))\n",
    "plt.title('Leave-one-out CV R-squared equals %.3f' % rho**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BbGggX4xbN7q"
   },
   "source": [
    "Interesting!! This is example of what we might call loosely **overfitting** but more things are going on here (e.g. leverage). What should note is that at this *model complexity* the procedure we have followed to estimate the learning function has become very **unstable**: it is overly sensitive to a small change in the training data, and in particular to two data points \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZKB7WnUbN7q"
   },
   "source": [
    "#### Exercise 1\n",
    "Pick one of the two extremal points in the input space, the ones close to a y value of 0, learn the model and plot the solution for the leave-one-out case. See how the estimated function varies when each of the points are left out and compare with the estimated function when all the data are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 403,
     "status": "ok",
     "timestamp": 1644881581110,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "wkSF_g59bN7q",
    "outputId": "101e900b-f11f-4337-dd62-b4950d964d6d"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TgbTOqTbN7q"
   },
   "source": [
    "## The bias-variance tradeoff in Statistics and Machine Learning\n",
    "\n",
    "The picture tells it all! (Taken from Bishop's book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Upcstb04bN7r"
   },
   "source": [
    "<img src=\"https://datasciencebocconi.github.io/Images/linear_models/bias_variance_bishop.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "exu7jmr3bN7r"
   },
   "source": [
    "This is typical: \n",
    "\n",
    "+ *Procedures* with *few degrees of freedom* are stable but the learning function they estimate can be systematically far off from the optimal one (**bias**). They would have comparable R-squared and leave-one-out CV R-squared\n",
    "\n",
    "+ *Procedures* with *high degrees of freedom* are sensitive to training data (**variance**) but the learning function they estimate might not have systematic differences from the optimal one. They would have near-1 R-squared and near-0 leave-one-out CV R-squared \n",
    "\n",
    "It is important to understand that these properties involve **both the model and the loss function** - this is why I tactically used the vague term *procedure* above: it is the combination of both - what we might call \"algorithm\" - that matters\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a63vu-1fbN7r"
   },
   "source": [
    "The following figure, taken also from Bishop, shows the estimated mean squared error - blue is in-sample, red is analogous to leave-one-out CV - for increasing values of $p$ (denoted by $M$ in the fig). \n",
    "\n",
    "<img src=\"https://datasciencebocconi.github.io/Images/linear_models/bishop_overfit.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKm02bAabN7r"
   },
   "source": [
    "We want **algorithms that can strike a good bias-variance tradeoff**!\n",
    "\n",
    "The remaining of this lecture is devoted to:\n",
    "\n",
    "1. Giving such algorithms: e.g. the so-called LASSO; they use the same linear-in-features model but a different loss function\n",
    "2. Discussing how to estimate $MSE_n$ from data; we will revisit CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benign overfitting\n",
    "\n",
    "Although beyond the aims of this notebook it is worth pondering for a moment how big models (large $p$) and big data (large $n$) challenge somewhat the perception on bias-variance tradeoff communicated by the picture above. Consider the following picture from the work of Mei and Montanari; $d$ is the ambient dimension, i.e., dim of inputs $\\bx$, $N$ is the number of features, i.e., the dim of $\\bphi$ ($p$ in our notation), and $n$ the number of data points. The solid lines correspond to an exact calculation of MSE in an asymptotic regime where $d,n,N\\to \\infty$\n",
    "\n",
    "<img src=\"https://datasciencebocconi.github.io/Images/linear_models/double_descent.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ig9_rTr5bN7r"
   },
   "source": [
    "## A framework for good predictive algorithms: selection and shrinkage\n",
    "\n",
    "We now see *algorithms* that achieve a good bias-variance tradeoff and allow us to fit linear models with very large number of features, even much larger than the number of observations - e.g $p \\approx e^n$. The key structure that they try to exploit, and do well when this structure is consistent with the data, is that of **sparsity**, i.e., that only a small number of terms in the linear model are needed to get good predictions. The training will select the few important features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVwqSKKdbN7r"
   },
   "source": [
    "These *algorithms* use the *same linear models* we have seen before. But they use *different loss functions* \n",
    "\n",
    "We focus on the coefficients $\\bb$: since their estimation did not require knowledge of $v$, we simplify (and rescale) the loss function we derived earlier to \n",
    "\n",
    "$$L(\\bb) =  {1 \\over 2 n} \\sum_{i=1}^n (y_i - f(\\bx_i,\\bb))^2$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One classic approach to building good predictive, but also **explainable** models is **best subset selection**. The idea is to fit linear models with different number of features and try and choose for any given number, say $k$, the best $k$ features. We score any model according to the loss function $L(\\bb)$; the challenge is to find for a given $k$ the best $k$ features because this requires considering $p$ choose $k$ models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an illustration from the Elements of Statistical Learning with p=8+intercept\n",
    "\n",
    "<img src=\"https://datasciencebocconi.github.io/Images/linear_models/best_subset_prostate.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best subset selection is feasible for small $p$ but it is practically infeasible for large $p$, say larger than 100. Actually, best subset selection is an NP-hard computational problem. What we do in practice is **heuristic optimization**, e.g. greedy-search. A common approach is **forward stepwise regression**. Note that we need something additional to **choose the best size** $k$, since for $L(\\bb)$ the larger $k$ the better. We can use e.g., LOO-CV as discussed earlier or the model selection criteria discussed below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVwqSKKdbN7r"
   },
   "source": [
    "An alternative class of algorithms we discuss now are in the family of so-called **shrinkage methods**; they are based on changing the loss function to \n",
    "\n",
    "$$L(\\bb) =  {1 \\over 2 n} \\sum_{i=1}^n (y_i - f(\\bx_i,\\bb))^2 + \\lambda \\sum_{j=0}^{p-1} g(\\beta_j)$$ \n",
    "\n",
    "where $g(\\beta_j)$ is a **penalty** term, that penalizes $\\beta_j$ when $\\beta_j \\neq  0$; recall that $\\beta_j = 0$ means that feature $j$ (e.g., $j-1$ polynomial order) is dropped from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xd9FeQRZbN7r"
   },
   "source": [
    "This family of loss functions are called penalized likelihood. The following are some common examples of penalties - and the names the corresponding algorithms are known with: \n",
    "\n",
    "+ LASSO: $g(\\beta) = |\\beta|$\n",
    "\n",
    "+ ridge regression: $g(\\beta) = \\beta^2$\n",
    "\n",
    "+ Elastic Net/SCAD/MC+/Reciprocal LASSO/...\n",
    "\n",
    "Again the picture says it all: \n",
    "\n",
    "<img src = \"https://datasciencebocconi.github.io/Images/linear_models/penalties.png\" width =\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WUI7V9wJbN7r"
   },
   "source": [
    "Some remarks: \n",
    "\n",
    "+ Feature standardization: \n",
    "    + Different coefficients are penalized in the same way: this only makes sense if the different coefficients are comparable. Consider the following silly example that makes the point: suppose I want to build a simple predictive model for the time it takes to get with my bike from my home to a given location in Barcelona just in terms of the vertical dispacement, $v$, and horizontal dispacement $h$, in terms of data $(y_i,v_i,h_i)$ of times it took in past trips $y_i$ when the displacements were $v_i$ and $h_i$. My house is on the beach and I mostly move along the coast, so I decided to record the horizontal discplacement in kilometers and the vertical dispacement in meters. My model is \n",
    "    $$ y_i = \\beta_1 h_i + \\beta_2 v_i + error$$\n",
    "        I should expect that $\\beta_1 \\approx 1000 \\beta_2$ - they will be on completely different scales.\n",
    "        \n",
    "    + Penalized likelihood algorithms require that the features have been standardized to have comparable scales. We often subtract the sample mean and divide by the standard deviation across replications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQgkjGy9bN7r"
   },
   "source": [
    "+ The role of $\\lambda$:\n",
    "    + This **hyperparameter** allows us to trade bias with variance, creating a continuum of mean squared errors along which we try to choose an optimal $\\lambda$ - hence an optimal predictive model. \n",
    "    + $\\lambda \\to 0$ leads to small bias/large variance, $\\lambda \\to \\infty$ to large bias/small variance. \n",
    "    + Lets revisit now the bias-variance tradeoff picture: ridge regression with varying $\\lambda$s: \n",
    "    <img src=\"https://datasciencebocconi.github.io/Images/linear_models/bias_variance_bishop.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3C1TTjpObN7r"
   },
   "source": [
    "### Lasso in action: the curve data with many many features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1644881590531,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "AthNmMzPbN7r"
   },
   "outputs": [],
   "source": [
    "order = 9\n",
    "poly = plf(order)\n",
    "Flarge = poly.fit_transform(X)[:,1:] # drop the intercept column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.around(Flarge,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1644881590531,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "AthNmMzPbN7r"
   },
   "outputs": [],
   "source": [
    "# standardisation of input is critical: We will use sklearn to do this\n",
    "\n",
    "# generic lasso regression object\n",
    "from sklearn.preprocessing import scale as scl\n",
    "Flarge = scl(Flarge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.around(Flarge,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1644881593079,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "rZy1TccfbN7r",
    "outputId": "779418b7-807e-4824-eac0-c1b95aff8c3e"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "#alpha is what was lambda in our notation\n",
    "regr_lasso = Lasso(alpha=0.0001, fit_intercept=True,warm_start=True,max_iter=100000)\n",
    "\n",
    "# application to our data and model\n",
    "regr_lasso.fit(Flarge,y)\n",
    "\n",
    "# see coefficients\n",
    "print(regr_lasso.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "executionInfo": {
     "elapsed": 523,
     "status": "ok",
     "timestamp": 1644881595961,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "aiYzYPqIbN7r",
    "outputId": "79c5097c-d3ff-40b8-88a9-9c4f2fb8fbfd"
   },
   "outputs": [],
   "source": [
    "# CV R2 \n",
    "y_hat_cv = cvp(regr_lasso, F, y, cv=10)\n",
    "plt.figure()\n",
    "plt.scatter(x=y,y=y_hat_cv) \n",
    "plt.plot(y,y,c=\"red\")\n",
    "rho = pd.Series(y[:,0]).corr(pd.Series(y_hat_cv))\n",
    "plt.title('R-squared equals %.3f' %rho**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7kUPAxhbN7r"
   },
   "source": [
    "### Further insights & observations on lasso\n",
    "\n",
    "+ Sparsity: increasing values of $\\lambda$ have the effect that an increasing number of estimated coefficients are exactly zero\n",
    "+ Variable selection: hence, implictly lasso also performs a principled feature selection - but this is not an aspect we will explore here\n",
    "    + Lets see these properties in action in our example. Lets look at the coefficients for a range of $\\lambda$ values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "executionInfo": {
     "elapsed": 671,
     "status": "ok",
     "timestamp": 1644881599363,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "Qq8CyayBbN7r",
    "outputId": "dcbb7965-a32b-47d4-c95f-cbd8cd2441b7"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import lars_path \n",
    "\n",
    "alphas, _, coefs = lars_path(Flarge, y[:,0], method='lasso', \n",
    "                             verbose=True, max_iter = 100000)\n",
    "xx = np.sum(np.abs(coefs.T), axis=1)\n",
    "xx /= xx[-1]\n",
    "plt.plot(xx, coefs.T)\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.vlines(xx, ymin, ymax, linestyle='dashed')\n",
    "plt.xlabel('|coef| / max|coef|')\n",
    "plt.ylabel('Coefficients')\n",
    "plt.title('LASSO Path')\n",
    "plt.axis('tight')\n",
    "plt.xlim(0,0.01)\n",
    "plt.ylim(-3,3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7qDRm5pbN7r"
   },
   "source": [
    "+ Convexity: the loss function is convex; this is because the least squares function is convex (a quadratic function) and the penalty is convex too. This allows very efficient estimation using **convex optimization** algorithms. \n",
    "    + A common choice is **coordinate-wise descent**. This is an iterative algorithm that scans through each coefficient and updates it using information about the values of all other coefficients. \n",
    "    + For standardized features $\\bx_1,\\bx_2,\\ldots$ each coefficient is updated as: \n",
    "$$\n",
    "\\beta_j \\leftarrow \\mathcal{S}_{\\lambda}\\left({1 \\over n} \\boldsymbol{r}_{-j}^T \\bx_j\\right )\n",
    "$$\n",
    "where $\\boldsymbol{r}_{-j}$ is the vector of residuals from the model with $\\beta_j = 0$ and the soft-thresholding operator is:\n",
    "$$\n",
    "\\mathcal{S}_{\\lambda}(\\beta) = \\mathrm{sign}(\\beta) \\max\\{|\\beta| - \\lambda,0\\}$$\n",
    "    + The fast optimization is a major attraction for the lasso \n",
    "      + Coordinate-wise descent is implemented at a cost that grows only linearly in $n$ and $p$: it is a practical solution for Big Data and Big Models  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_y0KrS4kbN7r"
   },
   "source": [
    "## Choosing the regularization hyperparameter\n",
    "\n",
    "We want to choose $\\lambda$ to a good MSE, that is a good bias-variance tradeoff and as a result good *predictive performance*. This is a **model complexity** choice, effectively. \n",
    "\n",
    "A problem identified earlier is that the so-called *in-sample* MSE (or equivalently $R^2$) is biased for the population MSE that we wish to estimate, in particular it is *optimistic*. \n",
    "\n",
    "There are at least two ways to deal with this issue: \n",
    "+ Use **cross-validation** (CV) to estimate the MSE. We have seen leave-one-out CV already and we experiment with this and other versions below and give some insights. $K$-fold CV splits the data into $K$ groups and use all but one group for learning and the remaining for estimating the MSE, and averages the $K$ resultant estimates. Leave-one-out CV is using $n$-folds.  \n",
    "  + The advantages of CV:\n",
    "      + It is generic and can be applied to general models and for criteria other than MSE\n",
    "  + The disadvantes of CV:\n",
    "      + It is computationally demanding, especially when it has to be performed several times when tuning for a hyperparameter. Additionally there is not much understanding on what CV to use, that is how many splits to use on the data\n",
    "+ Use a method that does a bias-correction to remove the optimism. There are at least two ways to do this. One is based on the bootstrap and it also uses simulation. The other is based on more assumptions and mathematical calculations and relates to the so-called **information criteria** (IC). We also consider those below\n",
    "  + The advantages of IC:\n",
    "      + It is computationally very efficient, and for this reason it is often the default choice in practice\n",
    "  + The disadvantes of IC:\n",
    "      + They rely on more assumptions and asymptotic approximations and lack the generic applicability of CV\n",
    "\n",
    "One interesting connection between the two paradigms is that for linear models, estimated by OLS, asymptotically (for large $n$) leave-one-out CV is the optimal way to use CV and it is equivalent to the AIC, an IC we discuss below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nk38wMf4bN7r"
   },
   "source": [
    "### Leave-one-out CV selection of $\\lambda$ for the curve data\n",
    "\n",
    "Lets try and do this using the leave-one-out CV we have already discussed. We will try a range of different $\\lambda$s, for each of which we will estimate the MSE by leave-one-out CV, plot the resultant curve and try to identify a good $\\lambda$\n",
    "\n",
    "The procedure is computationally intensive - this will not manifest here where $n=10$\n",
    "\n",
    "We will use `GridSearchCV` to carry out the outer (grid search and CV) loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1644881620147,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "InnCh933bN7r"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# you may wonder why I set random_state now and did not do so before\n",
    "lasso = Lasso(random_state=0,max_iter=3000000) \n",
    "alphas = np.array([0.000007, 0.00002, 0.00004, 0.00005,0.00008,0.0001,0.00012, 0.00015,0.0002,0.00025,0.0003,0.0004,0.0005,0.0006,0.0007,0.002])\n",
    "\n",
    "tuned_parameters = [{'alpha': alphas}]\n",
    "n_folds = 10 # remember that for this dataset this is leave-one-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13927,
     "status": "ok",
     "timestamp": 1644881635357,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "_00F6Ti7bN7r"
   },
   "outputs": [],
   "source": [
    "# create a scorer to evaluate performance\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, make_scorer \n",
    "\n",
    "## ALWAYS read carefully documentation. copying here from make_scorer\n",
    "## greater_is_better : boolean, default=True\n",
    "# \"Whether score_func is a score function (default), meaning high is \n",
    "# good, or a loss function, meaning low is good. \n",
    "# In the latter case, the scorer object will sign-flip \n",
    "# the outcome of the score_func.\n",
    "mse = make_scorer(mean_squared_error,greater_is_better=False)\n",
    "\n",
    "\n",
    "clf = GridSearchCV(lasso, tuned_parameters, scoring = mse, \n",
    "                   cv=n_folds, refit=False)\n",
    "\n",
    "clf.fit(Flarge, y)\n",
    "scores = clf.cv_results_['mean_test_score']\n",
    "scores_std = clf.cv_results_['std_test_score']\n",
    "std_error = scores_std / np.sqrt(n_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "executionInfo": {
     "elapsed": 1117,
     "status": "ok",
     "timestamp": 1644881640393,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "Vljj2LJMbN7r",
    "outputId": "96e42a6d-601f-4632-8fea-ae2d5ae26d30"
   },
   "outputs": [],
   "source": [
    "# Plotting the results\n",
    "\n",
    "plt.figure().set_size_inches(8, 6)\n",
    "plt.semilogx(alphas, scores)\n",
    "\n",
    "# plot error lines showing +/- std. errors of the scores\n",
    "plt.semilogx(alphas, scores + std_error, 'b--')\n",
    "plt.semilogx(alphas, scores - std_error, 'b--')\n",
    "\n",
    "# alpha=0.2 controls the translucency of the fill color\n",
    "plt.fill_between(alphas, scores + std_error, scores - std_error, alpha=0.2)\n",
    "\n",
    "plt.ylabel('CV score +/- std error')\n",
    "plt.xlabel('alpha')\n",
    "plt.axhline(np.max(scores), linestyle='--', color='.5')\n",
    "ymin, ymax = plt.ylim()\n",
    "plt.vlines([0.00003] ,ymin, ymax, linestyle='dashed')\n",
    "plt.xlim([alphas[0], alphas[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mvm8LbXqbN7r"
   },
   "source": [
    "#### Exercise 2\n",
    "Refit the lasso with the regularised *alpha* ($\\lambda$) parameter that we just found that maximizes the CV score. Check the number of parameters that have been shrunk to zero.\n",
    "\n",
    "Report the leave-one-out CV $R^2$ coefficient. Plot leave-one-out  predicted versus actual output values \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 238,
     "status": "ok",
     "timestamp": 1644881643903,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "1rNwV7U-bN7r",
    "outputId": "fb221bf9-3365-4b58-e97a-2d001268bbc2"
   },
   "outputs": [],
   "source": [
    "# Refitting the lasso with that regularising parameter\n",
    "# Hint: use Lasso()\n",
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "executionInfo": {
     "elapsed": 567,
     "status": "ok",
     "timestamp": 1644881647136,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "kC6OFzkhbN7r",
    "outputId": "c9c593bd-1c08-4754-a075-c511fefc40f4"
   },
   "outputs": [],
   "source": [
    "# CV R2 and plot \n",
    "# Hint: Use cvp() on the previous model, then plot y versus y_hat_cv as we did before\n",
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CEwNnFybZ6t"
   },
   "source": [
    "#### Exercise 3\n",
    "\n",
    "Repeat the experiment with 3-fold CV, and refit the lasso with the regularised *alpha* ($\\lambda$) parameter that found. Check the number of parameters that have been shrunk to zero.\n",
    "\n",
    "Report the leave-one-out CV $R^2$ coefficient. Plot leave-one-out  predicted versus actual output values. \n",
    "\n",
    "What are your conclusions? Can you explain what you see on the basis of our previous experiments in this notebook?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "executionInfo": {
     "elapsed": 1142,
     "status": "ok",
     "timestamp": 1644881651278,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "ehGsKJngbZ6t",
    "outputId": "86a40162-d17d-4b3a-d830-199945df6952"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3AeQK8l2bZ6u"
   },
   "source": [
    "### Information criteria\n",
    "\n",
    "Information criteria do a bias-correction on the maximised likelihood. In the context of linear regression this is effectively the same as a bias-correction to the in-sample MSE. One example is the so-called Akaike Information Criterion (AIC) and its variations. For linear models this is equivalent to another model selection criterion, the so-called Mallows's $C_p$. \n",
    "\n",
    "\n",
    "For a statistical model that involves $p$ parameters and loss function which is the log-likelihood, the AIC takes the form: \n",
    "\n",
    "$$AIC(\\textrm{model}) = -{2 \\over n} \\textrm{(maximised log-lik of model)} + {2 \\over n} p$$\n",
    "\n",
    "When we deviate from maximum likelihood, the *effective number of parameters* - we will call this **degrees of freedom (df)** differs from $p$. In particular: \n",
    "\n",
    "+ Algorithms that have used the response data to select the model, they will have $df \\geq p$: effectively more than $p$ parameters have been used to come up with this particular model\n",
    "+ Algorithms that shrink the parameter values towards zero, they will have $df \\leq p$: effectively each parameter counts less - or even 0 if it is shrunk exactly to 0 \n",
    "\n",
    "Specifically to the case where there is a family of algorithms indexed by a tuning parameter $\\lambda$ (e.g. the regularizing parameter) and AIC is used to score each member of the family, the AIC formula becomes: \n",
    "\n",
    "$$AIC(\\lambda) = -{2 \\over n} \\textrm{(maximised log-lik of model for given }\\lambda) + {2 \\over n} df(\\lambda)$$\n",
    "\n",
    "where $df(\\lambda)$ are the degrees of freedom for that particular value of the tuning parameter. \n",
    "\n",
    "The name of the game is to come up with an estimate of $df(\\lambda)$ for the algorithm of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPeofiiUbZ6u"
   },
   "source": [
    "#### Degrees of freedom of lasso\n",
    "\n",
    "Rather surprisingly the degrees of freedom for a given value of $\\lambda$ used in a lasso algorithm is *unbiasedly estimated* by the resultant number of non-zero coefficients. \n",
    "\n",
    "For the sake of clarity, let \n",
    "$m_\\lambda$ be the size of the model chosen with $\\lambda$ regularizer value, and $\\lambda_m$ the *transition point* in the lasso path (from empty to full) at which the model size becomes $m$ \n",
    "\n",
    "The result for the lasso is that $m_\\lambda$ is an unbiased estimator of $df(\\lambda)$ - this is established in Zou, Hastie and Tibshirani (2007, Annals of Statistics)\n",
    "\n",
    "+ Lasso uses the response data to choose the model, hence we would expect $df(\\lambda) \\geq m_\\lambda$ \n",
    "+ On the other hand, lasso shrinks coefficients to zero hence we would expect $df(\\lambda) \\leq m_\\lambda$\n",
    "+ Magically, the two effects cancel out and $m_\\lambda$ is unbiased for $df(\\lambda)$\n",
    "\n",
    "Combined with the Gaussian likelihood, the AIC criterion for lasso becomes \n",
    "\n",
    "$$AIC(\\lambda) = {\\sum_i (y_i - \\hf(\\bx_i))^2 \\over n v} + {2 \\over n} m_\\lambda$$\n",
    "\n",
    "An important result is that it is only necessary to search among the transition points $\\lambda_1,\\lambda_2,\\ldots$ to identify the model that optimizes the AIC criterion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwYHYm3KbN7r"
   },
   "source": [
    "## Some hints for practitioners\n",
    "\n",
    "\n",
    "+ Building good predictive models with hundreds or even thousands of features is a real possibility\n",
    "+ LASSO combines least squares fit with a penalty for model complexity; it relies on an additional *regularizing hyperparameter*\n",
    "+ Sklearn module `LinearRegression` can be used for predictive modelling. `Lasso` can be used to fit a lasso model for given value of regularization hyperparameter. `lars_path` can return all the possible lasso solutions for all values of the regularization hyperparameter and is a useful tool in exploring the different models\n",
    "+ The choice of regularization hyperparameter is a model choice problem; you can use both cross validation to estimate the MSE for each possible value of the hyperparameter and use a grid search to identify good values for the hyperparameter - `GridSearchCV` is useful wrapper for this. Less data and computationally intensive method is to use a model selection criterion, e.g. AIC, and a simple formula exists for the lasso\n",
    "+ Cross validation is based on splitting the available data in $K$ groups, training with data in all but one groups, evaluating a performance criterion on the held out group, and averaging the evaluation estimates across groups. $K=n$ corresponds to leave-one-out CV; this is computationally very demanding. Common choices are $K=5$ or $K=10$ but a good choice of $K$ is largely a mystery. Be careful with how to properly work with CV-calibrated algorithms, read Section 7.2 of Hastie et al. referenced below\n",
    "+ For inference with a linear model, i.e., obtaining confidence intervals, p-values etc, `LinearRegression` is  entirely inappropriate. Use other modules, e.g., `statsmodels.api`. \n",
    "+ Inference with the output of the lasso model is non-trivial and subject of more advanced material. Although lasso implictly selects a model by dropping variables, you should not over-interpret the variables that have been selected. Its merit is primary in getting a good predictive model. Lasso is helpful in screening some variables, so it is often used as a first step to be followed by a more formal selection procedure. Generally, these questions fall under the theme of *post-selection* inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1XKjrE7bN7r"
   },
   "source": [
    "## References\n",
    "\n",
    "Hastie, T., Tibshirani, R., Friedman, J., 2009. *Elements of Statistical Learning*. 2nd Edition. Chapters 1, 2, Section 3.4; More advanced 3.8,3.9,7.10  https://web.stanford.edu/~hastie/ElemStatLearn/\n",
    "\n",
    "Bishop, C.M. *Pattern recognition and machine learning*. Chapter 1, Sections 3.1, 3.2\n",
    "\n",
    "Arlot, S., Celisse, A. 2010. *A survey of cross-validation procedures for model selection* https://projecteuclid.org/journals/statistics-surveys/volume-4/issue-none/A-survey-of-cross-validation-procedures-for-model-selection/10.1214/09-SS054.full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPJCAUwmbZ6v"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "linear_models_tutorcopy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
