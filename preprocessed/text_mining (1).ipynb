{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Case study: the IMDb dataset\n",
        "\n",
        "Foundations of Data Science\n",
        "\n",
        "## How to use this material\n",
        "\n",
        "-   The coding is an important part of this lecture and I will comment\n",
        "    on the most relevant aspects.\n",
        "\n",
        "-   The analyses made in these slides are fully\n",
        "    <span style=\"color:DarkOrange\">reproducible</span>. You are\n",
        "    encouraged to:\n",
        "\n",
        "    -   Create your own Python notebook and reproduce the results, using\n",
        "        the code provided in these slides.\n",
        "    -   Be independent. If something does not work, try to fix it before\n",
        "        asking.\n",
        "    -   Be creative. Modify the code and explore the data yourself.\n",
        "\n",
        "## Textual data\n",
        "\n",
        "-   Data cleaning and pre-processing\n",
        "    -   <span style=\"color:DarkOrange\">Cleaning the data</span>. Removal\n",
        "        of abbreviations, HTML/XML tags (if any), special symbols and\n",
        "        other jargons.\n",
        "    -   <span style=\"color:DarkOrange\">Tokenization and stemming</span>.\n",
        "        Tokenization means that we split sentences into “tokens”, i.e.\n",
        "        words. Stemming means that words are reduced to their root, i.e.\n",
        "        without suffixes.\n",
        "-   Data analysis\n",
        "    -   <span style=\"color:DarkOrange\">Term document matrix\n",
        "        (TDM)</span>. Textual data are converted into a matrix with\n",
        "        numeric entries. Frequency and related quantities (tf-idf) are\n",
        "        obtained.\n",
        "    -   <span style=\"color:DarkOrange\">Unsupervised sentiment\n",
        "        analysis</span>. Classification of the sentiment of each\n",
        "        document using dictionaries and no human intervention.\n",
        "\n",
        "## Getting started\n",
        "\n",
        "-   In the first place, let us load the relevant python packages.\n",
        "\n",
        "-   The [`nltk` (Natural Language\n",
        "    Toolkit)](https://www.nltk.org/index.html) package is the main tool\n",
        "    we will use in this notebook.\n",
        "\n",
        "-   You should be familiar with this package with `pandas` at this\n",
        "    stage."
      ],
      "id": "ecf6b9bc-c2c2-48c6-ae63-d2e01769761f"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Loading the necessary packages\n",
        "import pandas as pd\n",
        "import re # Package for regular expressions\n",
        "import nltk # Main python package for natural language processing"
      ],
      "id": "fc51ce41"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## String manipulation I\n",
        "\n",
        "-   Let us recap some basic notions about\n",
        "    <span style=\"color:DarkOrange\">strings</span>.\n",
        "\n",
        "-   An example of a string is the following:"
      ],
      "id": "2237d219-9cd3-4042-8c87-20ade2e8082c"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "'Monty Python'"
            ]
          }
        }
      ],
      "source": [
        "monty = 'Monty Python'\n",
        "monty"
      ],
      "id": "9cf67709"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   You can use the `\"` symbol instead of `'`, which is necessary when\n",
        "    handling with the apostrophe."
      ],
      "id": "d2646e58-4b9c-49a7-b890-c66324879b7e"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "\"Monty Python's Flying Circus\""
            ]
          }
        }
      ],
      "source": [
        "circus = \"Monty Python's Flying Circus\"\n",
        "circus"
      ],
      "id": "2bb86769"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## String manipulation II\n",
        "\n",
        "-   An alternative syntax is the following:"
      ],
      "id": "d5ad7f1d-c46e-4c43-ac02-8fa8dfe64b0c"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "\"Monty Python's Flying Circus\""
            ]
          }
        }
      ],
      "source": [
        "circus = 'Monty Python\\'s Flying Circus'\n",
        "circus"
      ],
      "id": "4c0f89d0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Elements of a string can be accessed as if they were a list:"
      ],
      "id": "22bf0623-e93c-4fd0-951a-a59b6afff5fe"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "'M'"
            ]
          }
        }
      ],
      "source": [
        "monty[0] # First character of the string"
      ],
      "id": "767f8a03"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "'Monty'"
            ]
          }
        }
      ],
      "source": [
        "monty[0:5] # From 0 to 5 (not including 5)"
      ],
      "id": "4c86c4ba"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## String manipulation III\n",
        "\n",
        "-   Strings can be combined using the `+` operator, and repeated using\n",
        "    the `*` operator:"
      ],
      "id": "0e870b1c-2ac6-49b5-b5f9-f4828ecf6f32"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "'hellovery much'"
            ]
          }
        }
      ],
      "source": [
        "'hello' + 'very' + ' ' + 'much'"
      ],
      "id": "04c0933b"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "'veryveryvery'"
            ]
          }
        }
      ],
      "source": [
        "'very' * 3"
      ],
      "id": "ca250f44"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   A string can be split, using space as the separator."
      ],
      "id": "27b02b52-96c4-46e2-abea-7ed24bb501f2"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "['Monty', 'Python']"
            ]
          }
        }
      ],
      "source": [
        "monty_list = monty.split()\n",
        "monty_list"
      ],
      "id": "148d230a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   This is similar in spirit to the\n",
        "    <span style=\"color:DarkOrange\">tokenization</span>.\n",
        "\n",
        "-   A list containing strings can be joined into a single string, using\n",
        "    the following syntax:"
      ],
      "id": "0f2a6c8f-299d-4470-842c-b4b3e3590a8f"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "'Monty Python'"
            ]
          }
        }
      ],
      "source": [
        "\" \".join(monty_list)"
      ],
      "id": "1350f920"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regular expressions\n",
        "\n",
        "-   <span style=\"color:DarkOrange\">Regular expressions</span>, sometimes\n",
        "    shortened in “regex”, are a powerful and flexible method for\n",
        "    specifying **search patterns**.\n",
        "\n",
        "-   An example of regular expression is `([A-z])+`.\n",
        "\n",
        "-   To use regular expressions in python, we need to use the package\n",
        "    `re`.\n",
        "\n",
        "-   There are several online resources about\n",
        "    <span style=\"color:DarkOrange\">regex</span>.\n",
        "\n",
        "-   If you want to play around with this kind of syntax, you can visit\n",
        "    website <https://regexr.com>.\n",
        "\n",
        "-   We will not discuss regular expressions in this notebook, but it is\n",
        "    essential to keep in mind that they are often at the core of more\n",
        "    advanced functions.\n",
        "\n",
        "## The IMDb dataset\n",
        "\n",
        "-   In this notebook, we will analyze a\n",
        "    <span style=\"color:DarkOrange\">subset</span> of the [**Large Movie\n",
        "    Review Dataset**](https://ai.stanford.edu/~amaas/data/sentiment/).\n",
        "\n",
        "-   This dataset is associated with the paper\n",
        "\n",
        "> Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y.\n",
        "> Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment\n",
        "> Analysis. *The 49th Annual Meeting of the Association for\n",
        "> Computational Linguistics (ACL 2011)*.\n",
        "\n",
        "-   The `IMDB_small.csv` dataset contains only `200` movie reviews. The\n",
        "    original dataset is much bigger.\n",
        "\n",
        "-   Let us import it into python, using `pandas`:"
      ],
      "id": "44e2b073-2a3d-459e-a07b-eeda8a0c34a3"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "(200, 1)"
            ]
          }
        }
      ],
      "source": [
        "# Let us download the dataset from the course repository\n",
        "imdb = pd.read_csv('https://datasciencebocconi.github.io/Data/IMDB_small.csv')\n",
        "imdb.shape # Size of the dataset"
      ],
      "id": "053c8f50"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The IMDb dataset\n",
        "\n",
        "![](https://datasciencebocconi.github.io/Images/text_mining/imdb.png)\n",
        "\n",
        "## A glimpse of the data\n",
        "\n",
        "-   The first `5` reviews (out of `200`) can be displayed in the\n",
        "    following:"
      ],
      "id": "f845acad-b9c0-45e1-87c1-732ac22a8a88"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "# Display the first 5 rows of this dataset\n",
        "imdb.head(5)"
      ],
      "id": "255dbf84"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   <span style=\"color:DarkOrange\">Disclaimer</span>. These are\n",
        "    <span style=\"color:DarkOrange\">real movie reviews</span> written by\n",
        "    anonymous people over the internet, which means there might be\n",
        "    <span style=\"color:DarkOrange\">offensive words</span>.\n",
        "\n",
        "## Document term matrix\n",
        "\n",
        "-   We wish to transform the data into something like this:\n",
        "\n",
        "| Document   | Word 1   | Word 2   | Word 3   | …       | Word $p-1$    | Word $p$ |\n",
        "|------------|----------|----------|----------|---------|---------------|----------|\n",
        "| Review 1   | $n_{11}$ | $n_{12}$ | $n_{13}$ | $\\dots$ | $n_{1,{p-1}}$ | $n_{1p}$ |\n",
        "| Review 2   | $n_{21}$ | $n_{22}$ | $n_{23}$ |         | $n_{2,{p-1}}$ | $n_{2p}$ |\n",
        "| $\\vdots$   | $\\vdots$ | $\\vdots$ | $\\vdots$ |         | $\\vdots$      | $\\vdots$ |\n",
        "| Review $N$ | $n_{N1}$ | $n_{N1}$ | $n_{N3}$ | $\\dots$ | $n_{N,p-1}$   | $n_{Np}$ |\n",
        "\n",
        "-   Each $n_{ij}$ is the number of times the $j$th word appears in the\n",
        "    $i$th review.\n",
        "\n",
        "-   This object is sometimes called\n",
        "    <span style=\"color:DarkOrange\">document term matrix</span> and it is\n",
        "    the starting point of most analyses.\n",
        "\n",
        "-   This is a deceptively simple problem: in practice, it requires a lot\n",
        "    of <span style=\"color:DarkOrange\">pre-processing</span>.\n",
        "\n",
        "-   A <span style=\"color:DarkOrange\">bag of words</span>. What is the\n",
        "    implicit assumption behind this representation?\n",
        "\n",
        "## HTML and abbreviations I\n",
        "\n",
        "-   Let us look at a random review, say the 9th."
      ],
      "id": "ee9c7c1d-878b-4b00-a72a-556f2c140a4f"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "\"Encouraged by the positive comments about this film on here I was looking forward to watching this film. Bad mistake. I've seen 950+ films and this is truly one of the worst of them - it's awful in almost every way: editing, pacing, storyline, 'acting,' soundtrack (the film's only song - a lame country tune - is played no less than four times). The film looks cheap and nasty and is boring in the extreme. Rarely have I been so happy to see the end credits of a film. <br /><br />The only thing that prevents me giving this a 1-score is Harvey Keitel - while this is far from his best performance he at least seems to be making a bit of an effort. One for Keitel obsessives only.\""
            ]
          }
        }
      ],
      "source": [
        "review = imdb.iloc[8, 0]\n",
        "review"
      ],
      "id": "931682b2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   It is not the most positive review ever written :-)\n",
        "\n",
        "-   Let us focus on the technical aspects at the moment.\n",
        "\n",
        "## HTML and abbreviations II\n",
        "\n",
        "-   There are several weird `<br />` symbols, which are\n",
        "    <span style=\"color:DarkOrange\">HTML tags</span>.\n",
        "\n",
        "-   In fact, these movie reviews have been downloaded from the [IMDB\n",
        "    website](https://www.imdb.com).\n",
        "\n",
        "-   These tags are not informative, so we need to remove them. A first\n",
        "    approach is using <span style=\"color:DarkOrange\">regular\n",
        "    expressions</span>.\n",
        "\n",
        "-   The following command replaces `<br />` with a blank space."
      ],
      "id": "d6ce30cb-ed8b-4ad9-a50c-32761692e983"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "\"Encouraged by the positive comments about this film on here I was looking forward to watching this film. Bad mistake. I've seen 950+ films and this is truly one of the worst of them - it's awful in almost every way: editing, pacing, storyline, 'acting,' soundtrack (the film's only song - a lame country tune - is played no less than four times). The film looks cheap and nasty and is boring in the extreme. Rarely have I been so happy to see the end credits of a film.   The only thing that prevents me giving this a 1-score is Harvey Keitel - while this is far from his best performance he at least seems to be making a bit of an effort. One for Keitel obsessives only.\""
            ]
          }
        }
      ],
      "source": [
        "re.sub(r\"<br />\", \" \", review) # Removes the <br /> tag"
      ],
      "id": "f435221e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HTML and abbreviations III\n",
        "\n",
        "-   Albeit useful, the above regular expression fixes only a very\n",
        "    specific HTML tag.\n",
        "\n",
        "-   To remove all the HTML parts of the text, we need a\n",
        "    <span style=\"color:DarkOrange\">dictionary</span>.\n",
        "\n",
        "-   Here, we make use of the `BeautifulSoup` package, whose\n",
        "    [documentation](https://beautiful-soup-4.readthedocs.io/en/latest/)\n",
        "    is available online."
      ],
      "id": "ce9debfe-6ee7-4821-adcf-1566448154ab"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "\"Encouraged by the positive comments about this film on here I was looking forward to watching this film. Bad mistake. I've seen 950+ films and this is truly one of the worst of them - it's awful in almost every way: editing, pacing, storyline, 'acting,' soundtrack (the film's only song - a lame country tune - is played no less than four times). The film looks cheap and nasty and is boring in the extreme. Rarely have I been so happy to see the end credits of a film. The only thing that prevents me giving this a 1-score is Harvey Keitel - while this is far from his best performance he at least seems to be making a bit of an effort. One for Keitel obsessives only.\""
            ]
          }
        }
      ],
      "source": [
        "from bs4 import BeautifulSoup # Load the package\n",
        "\n",
        "# Removes the <br /> and other HTML tags\n",
        "def remove_html(data):\n",
        "    data = BeautifulSoup(data)\n",
        "    return data.getText()\n",
        "\n",
        "review = remove_html(review)\n",
        "review"
      ],
      "id": "aae2c1b4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## English abbreviations\n",
        "\n",
        "-   Another source of concern is the presence of standard English\n",
        "    abbreviations, which we want to replace with their extended form.\n",
        "\n",
        "-   We can do this by defining our own\n",
        "    <span style=\"color:DarkOrange\">dictionary</span>.\n",
        "\n",
        "-   The following dictionary is by no means exhaustive. Feel free to\n",
        "    modify it and add other examples."
      ],
      "id": "8b5d3257-080f-4ee3-aa14-ac188f551d49"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_abb(review):\n",
        "    replacements = {\n",
        "       \"ain't\": \"am not\",\n",
        "        \"aren't\": \"are not\",\n",
        "        \"can't\": \"cannot\",\n",
        "        \"could've\": \"could have\",\n",
        "        \"couldn't\": \"could not\",\n",
        "        \"didn't\": \"did not\",\n",
        "        \"doesn't\": \"does not\",\n",
        "        \"don't\": \"do not\",\n",
        "        \"gonna\": \"going to\",\n",
        "        \"hadn't\": \"had not\",\n",
        "        \"hasn't\": \"has not\",\n",
        "        \"haven't\": \"have not\",\n",
        "        \"he'd\": \"he would\",\n",
        "        \"he'll\": \"he will\",\n",
        "        \"he's\": \"he is\",\n",
        "        \"how'd\": \"how did\",\n",
        "        \"how'll\": \"how will\",\n",
        "        \"how's\": \"how is\",\n",
        "        \"I'd\": \"I would\",\n",
        "        \"I'll\": \"I will\",\n",
        "        \"I'm\": \"I am\",\n",
        "        \"I've\": \"I have\",\n",
        "        \"isn't\": \"is not\",\n",
        "        \"it'd\": \"it would\",\n",
        "        \"it'll\": \"it will\",\n",
        "        \"it's\": \"it is\",\n",
        "        \"Its\" : \"It is\",\n",
        "        \"let's\": \"let us\",\n",
        "        \"mightn't\": \"might not\",\n",
        "        \"mustn't\": \"must not\",\n",
        "        \"shan't\": \"shall not\",\n",
        "        \"she'd\": \"she would\",\n",
        "        \"she'll\": \"she will\",\n",
        "        \"she's\": \"she is\",\n",
        "        \"should've\": \"should have\",\n",
        "        \"shouldn't\": \"should not\",\n",
        "        \"that's\": \"that is\",\n",
        "        \"there's\": \"there is\",\n",
        "        \"they'd\": \"they would\",\n",
        "        \"wanna\" : \"want to\",\n",
        "        \"We're\" : \"We are\"\n",
        "    }\n",
        "    for key, value in replacements.items():\n",
        "        review = re.sub(r\"{}\".format(key), value, review)\n",
        "    return review"
      ],
      "id": "b27ad5d2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   We apply this function to the current review:"
      ],
      "id": "e4d2a30e-7175-400e-85ea-eec51dacefc2"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "\"Encouraged by the positive comments about this film on here I was looking forward to watching this film. Bad mistake. I have seen 950+ films and this is truly one of the worst of them - it is awful in almost every way: editing, pacing, storyline, 'acting,' soundtrack (the film's only song - a lame country tune - is played no less than four times). The film looks cheap and nasty and is boring in the extreme. Rarely have I been so happy to see the end credits of a film. The only thing that prevents me giving this a 1-score is Harvey Keitel - while this is far from his best performance he at least seems to be making a bit of an effort. One for Keitel obsessives only.\""
            ]
          }
        }
      ],
      "source": [
        "review = remove_abb(review)\n",
        "review"
      ],
      "id": "f6897235"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalization\n",
        "\n",
        "-   We now convert the text to the <span style=\"color:DarkOrange\">lower\n",
        "    case</span>.\n",
        "\n",
        "-   This can be done by using the `.lower()` method:"
      ],
      "id": "179e2369-6765-43ec-b667-3722a8204278"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "\"encouraged by the positive comments about this film on here i was looking forward to watching this film. bad mistake. i have seen 950+ films and this is truly one of the worst of them - it is awful in almost every way: editing, pacing, storyline, 'acting,' soundtrack (the film's only song - a lame country tune - is played no less than four times). the film looks cheap and nasty and is boring in the extreme. rarely have i been so happy to see the end credits of a film. the only thing that prevents me giving this a 1-score is harvey keitel - while this is far from his best performance he at least seems to be making a bit of an effort. one for keitel obsessives only.\""
            ]
          }
        }
      ],
      "source": [
        "review = review.lower()\n",
        "review"
      ],
      "id": "da6d8bce"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenization\n",
        "\n",
        "-   <span style=\"color:DarkOrange\">Tokenization</span> is the task of\n",
        "    cutting a string into linguistic units that constitute a piece of\n",
        "    language data.\n",
        "\n",
        "-   Tokenization is performed using specialized functions, such as the\n",
        "    `word_tokenize` of the `nltk` python package:"
      ],
      "id": "d99c69dd-0a59-4286-bc92-cba7780876d9"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "['one', 'for', 'keitel', 'obsessives', 'only', '.']"
            ]
          }
        }
      ],
      "source": [
        "review_tokens = nltk.word_tokenize(review) # Perform tokenization\n",
        "review_tokens[140:] # Shows the last tokens"
      ],
      "id": "28939b0a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Composite words (i.e., “San Siro”) should be treated as a single\n",
        "    token, but `word_tokenize` fails to recognize it.\n",
        "\n",
        "## Special symbols and punctuation\n",
        "\n",
        "-   In our analyses, we wish to focus on **words**, therefore we\n",
        "    **delete** commas, dots, and other special symbols such as `!@#*`.\n",
        "\n",
        "-   This is a simplifying operation because\n",
        "    <span style=\"color:DarkOrange\">punctuation</span> might be very\n",
        "    informative."
      ],
      "id": "f4a45f5e-4d6e-485c-b06d-e551cbc26814"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "['of', 'an', 'effort', 'one', 'for', 'keitel', 'obsessives', 'only']"
            ]
          }
        }
      ],
      "source": [
        "# Retain a word only if it is alphanumeric\n",
        "review_tokens = [words for words in review_tokens if words.isalpha()] \n",
        "review_tokens[115:]"
      ],
      "id": "6323cf13"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Filtering stopwords\n",
        "\n",
        "-   In many languages, there are\n",
        "    <span style=\"color:DarkOrange\">high-frequency words</span> that have\n",
        "    no meaning on their own, such as conjunctions and articles.\n",
        "\n",
        "-   These tokens are called\n",
        "    <span style=\"color:DarkOrange\">stopwords</span> and we wish to\n",
        "    eliminate them.\n",
        "\n",
        "-   A list of stopwords is conveniently stored in the `nltk.corpus`\n",
        "    package, as shown below"
      ],
      "id": "d44068db-74bd-4eb8-9a16-f76da3ae8db2"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          }
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords.words('english')[:10]"
      ],
      "id": "eef6ed43"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Stopwords can be removed as follows:"
      ],
      "id": "6822c805-2aa7-44db-b452-9e58c2672104"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "['encouraged', 'positive', 'comments', 'film', 'looking', 'forward']"
            ]
          }
        }
      ],
      "source": [
        "review_tokens = [words for words in review_tokens if words not in stopwords.words('english')]\n",
        "review_tokens[:6]"
      ],
      "id": "ec19391d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stemming I\n",
        "\n",
        "-   Stemming reduces each word to its\n",
        "    <span style=\"color:DarkOrange\">root</span>, namely deleting\n",
        "    suffixes, thus decreasing the dictionary and avoiding token\n",
        "    duplications.\n",
        "\n",
        "-   Stemming is performed using the `SnowballerStemmer` function.\n",
        "\n",
        "-   Other stemmers are available in the `nltk` package; please see the\n",
        "    [documentation](https://www.nltk.org/howto/stem.html) for further\n",
        "    info.\n",
        "\n",
        "-   Stemmers are language-dependent: we need to specify that the reviews\n",
        "    are written in English.\n",
        "\n",
        "## Stemming II\n",
        "\n",
        "-   Let us see what the effect of a stemmer on a couple of words is:"
      ],
      "id": "808a055b-fe46-4be5-a770-0474d06b69d1"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "'film'"
            ]
          }
        }
      ],
      "source": [
        "nltk.SnowballStemmer(\"english\").stem(\"films\")"
      ],
      "id": "a2c76664"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "'film'"
            ]
          }
        }
      ],
      "source": [
        "nltk.SnowballStemmer(\"english\").stem(\"filmed\")"
      ],
      "id": "b8a677e6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   We now perform stemming to the full review:"
      ],
      "id": "07b7f20d-9bc4-45fb-bb7c-a73ba8b60c7b"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "['encourag', 'posit', 'comment', 'film', 'look', 'forward', 'watch', 'film']"
            ]
          }
        }
      ],
      "source": [
        "review_tokens = [nltk.SnowballStemmer(\"english\").stem(words) for words in review_tokens]\n",
        "review_tokens[:8]"
      ],
      "id": "8a5135c1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Finally, we convert the tokens into a single string:"
      ],
      "id": "e55021e0-3fe9-4b1a-9fbe-5a576e1e117a"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "'encourag posit comment film look forward watch film bad mistak seen film truli one worst aw almost everi way edit pace storylin soundtrack film song lame countri tune play less four time film look cheap nasti bore extrem rare happi see end credit film thing prevent give harvey keitel far best perform least seem make bit effort one keitel obsess'"
            ]
          }
        }
      ],
      "source": [
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "detokenizer = TreebankWordDetokenizer()\n",
        "\n",
        "# Create a \"review\" from the stemmed tokens\n",
        "detokenizer.detokenize(review_tokens)"
      ],
      "id": "3f2740ed"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Putting pieces together"
      ],
      "id": "6341ad08-b2c2-4c6e-8cdb-ce84f099d4ec"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1st round of pre-processing\n",
        "def basic_cleaning(review):\n",
        "  review = remove_html(review) # Remove HTML\n",
        "  review = remove_abb(review) # Remove abbreviations\n",
        "  return review\n",
        "\n",
        "# 2nd round of Pre-processing\n",
        "def advanced_cleaning(review):\n",
        "  \n",
        "  # Basic cleaning (HTML + symbols)\n",
        "  review = basic_cleaning(review)\n",
        "  \n",
        "  # Normalization\n",
        "  review = review.lower()\n",
        "\n",
        "  # Tokenization\n",
        "  review_tokens = nltk.word_tokenize(review)\n",
        "  \n",
        "  # Special symbols and punctuation\n",
        "  review_tokens = [words for words in review_tokens if words.isalpha()] \n",
        "  \n",
        "  # Filtering\n",
        "  review_tokens = [words for words in review_tokens if words not in stopwords.words('english')]\n",
        "  \n",
        "  # Stemming\n",
        "  review_tokens = [nltk.SnowballStemmer(\"english\").stem(words) for words in review_tokens]\n",
        "  \n",
        "  # Conversion to a single string\n",
        "  review = detokenizer.detokenize(review_tokens)\n",
        "  return review"
      ],
      "id": "e8f88455"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Original document\n",
        "\n",
        "-   For instance, one has that the\n",
        "    <span style=\"color:DarkOrange\">original document</span> is:"
      ],
      "id": "eb6265b3-22e8-4bfd-98fa-ad91f0b5f34c"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "'Petter Mattei\\'s \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. <br /><br />This being a variation on the Arthur Schnitzler\\'s play about the same theme, the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact. Stylishly, the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.<br /><br />The only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits. A big city is not exactly the best place in which human relations find sincere fulfillment, as one discerns is the case with most of the people we encounter.<br /><br />The acting is good under Mr. Mattei\\'s direction. Steve Buscemi, Rosario Dawson, Carol Kane, Michael Imperioli, Adrian Grenier, and the rest of the talented cast, make these characters come alive.<br /><br />We wish Mr. Mattei good luck and await anxiously for his next work.'"
            ]
          }
        }
      ],
      "source": [
        "# Original document\n",
        "imdb.iloc[4,0]"
      ],
      "id": "033cb075"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic cleaning\n",
        "\n",
        "-   The document after <span style=\"color:DarkOrange\">basic\n",
        "    cleaning</span> is:"
      ],
      "id": "2cd522ab-fe7c-436a-a198-330c4247799d"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "'Petter Mattei\\'s \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. This being a variation on the Arthur Schnitzler\\'s play about the same theme, the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact. Stylishly, the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.The only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits. A big city is not exactly the best place in which human relations find sincere fulfillment, as one discerns is the case with most of the people we encounter.The acting is good under Mr. Mattei\\'s direction. Steve Buscemi, Rosario Dawson, Carol Kane, Michael Imperioli, Adrian Grenier, and the rest of the talented cast, make these characters come alive.We wish Mr. Mattei good luck and await anxiously for his next work.'"
            ]
          }
        }
      ],
      "source": [
        "# Basic cleaning\n",
        "basic_cleaning(imdb.iloc[4,0])"
      ],
      "id": "e18fdd74"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   The document after <span style=\"color:DarkOrange\">stemming</span>"
      ],
      "id": "214d2ca1-c322-43e3-80f2-123c1246deb1"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "'petter mattei love time money visual stun film watch mattei offer us vivid portrait human relat movi seem tell us money power success peopl differ situat encount variat arthur schnitzler play theme director transfer action present time new york differ charact meet connect one connect one way anoth next person one seem know previous point contact stylish film sophist luxuri look taken see peopl live world live thing one get soul pictur differ stage loneli one inhabit big citi exact best place human relat find sincer fulfil one discern case peopl act good mattei direct steve buscemi rosario dawson carol kane michael imperioli adrian grenier rest talent cast make charact come wish mattei good luck await anxious next work'"
            ]
          }
        }
      ],
      "source": [
        "# After stemming\n",
        "advanced_cleaning(imdb.iloc[4,0])"
      ],
      "id": "8c78a18d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Global operations\n",
        "\n",
        "-   Apply the `basic_cleaning` and `advanced_cleaning` to all the\n",
        "    reviews.\n",
        "\n",
        "-   Create two new variables in the dataset: `review_clean` and\n",
        "    `review_token`."
      ],
      "id": "866922c2-86fb-45ec-b5f6-46dae57cc4e9"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "# This could take a while\n",
        "imdb['review_clean'] = imdb['review'].apply(lambda z: basic_cleaning(z))\n",
        "imdb['review_token'] = imdb['review'].apply(lambda z: advanced_cleaning(z))\n",
        "\n",
        "imdb.head(2)"
      ],
      "id": "047f5803"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Term document matrix\n",
        "\n",
        "-   We want to know what are the <span style=\"color:DarkOrange\">most\n",
        "    common words</span>. This can quickly be done with the following\n",
        "    chunk of code:"
      ],
      "id": "e823e637-6546-4a41-a3f2-3768beb68433"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "(5294, 2)"
            ]
          }
        }
      ],
      "source": [
        "# Put everything into a single string\n",
        "words  = ' '.join(imdb['review_token'])\n",
        "# Create a global tokenization\n",
        "tokens = nltk.word_tokenize(words)\n",
        "\n",
        "# Conversion to \"text\"\n",
        "text = nltk.Text(tokens)\n",
        "# Compute the most common words\n",
        "fdist = nltk.FreqDist(text)\n",
        "\n",
        "# Use pandas for organizing and displaying the results\n",
        "df_words = pd.DataFrame(list(fdist.items()), columns = [\"Word\",\"Frequency\"])\n",
        "# Order words from the most frequent\n",
        "df_words = df_words.sort_values(by = \"Frequency\", ascending = False)\n",
        "\n",
        "# Dimension of the dataset\n",
        "df_words.shape"
      ],
      "id": "1f3a6725"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Hence, we obtained `5294` different stems.\n",
        "\n",
        "## Most frequent words"
      ],
      "id": "b4a2e556-792c-49fe-b2ca-5df878874dd0"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "df_words.head(10)"
      ],
      "id": "548f0bae"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Document term matrix I\n",
        "\n",
        "-   Our original goal was to obtain something like the following:\n",
        "\n",
        "| Document   | Word 1   | Word 2   | Word 3   | …       | Word $p-1$    | Word $p$ |\n",
        "|------------|----------|----------|----------|---------|---------------|----------|\n",
        "| Review 1   | $n_{11}$ | $n_{12}$ | $n_{13}$ | $\\dots$ | $n_{1,{p-1}}$ | $n_{1p}$ |\n",
        "| Review 2   | $n_{21}$ | $n_{22}$ | $n_{23}$ |         | $n_{2,{p-1}}$ | $n_{2p}$ |\n",
        "| $\\vdots$   | $\\vdots$ | $\\vdots$ | $\\vdots$ |         | $\\vdots$      | $\\vdots$ |\n",
        "| Review $N$ | $n_{N1}$ | $n_{N1}$ | $n_{N3}$ | $\\dots$ | $n_{N,p-1}$   | $n_{Np}$ |\n",
        "\n",
        "-   This is now an <span style=\"color:DarkOrange\">easy task</span>,\n",
        "    because documents (reviews) have been tokenized and stemmed.\n",
        "\n",
        "-   In practice, we will make use of the `CountVectorizer` of the\n",
        "    `sklearn` python package.\n",
        "\n",
        "## Document term matrix II\n",
        "\n",
        "-   The total number of distinct stems we obtained after cleaning is\n",
        "    `5294`.\n",
        "\n",
        "-   We consider only a fraction of them ($p = 500$): those having higher\n",
        "    frequencies."
      ],
      "id": "8a93c9d4-9276-49bf-8af6-717571c5830b"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Creation of a TDM with p = 500 words\n",
        "vectorizer = CountVectorizer(max_features = 500)\n",
        "X = vectorizer.fit_transform(imdb['review_token'])\n",
        "word_names = list(vectorizer.get_feature_names_out())\n",
        "\n",
        "# Conversion to dataframe\n",
        "X = pd.DataFrame(X.toarray())\n",
        "# Renaming columns according to words\n",
        "X.columns = word_names"
      ],
      "id": "89ef44ae"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   The `CountVectorizer` function performs more operations than we\n",
        "    need.\n",
        "\n",
        "-   For example, it silently converts the text to lowercase. It is also\n",
        "    possible to remove stopwords.\n",
        "\n",
        "-   These operations are <span style=\"color:DarkOrange\">redundant</span>\n",
        "    in our case.\n",
        "\n",
        "-   Please refer to the [official\n",
        "    documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)\n",
        "    for further details.\n",
        "\n",
        "## Document term matrix III"
      ],
      "id": "7131388e-8e22-4ecb-b09b-e0fe95c4324d"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "<p>8 rows × 500 columns</p>\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "X.head(8)"
      ],
      "id": "a89f60cb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF-IDF transformation I\n",
        "\n",
        "-   Sometimes, one might be interested in obtaining a variation of the\n",
        "    former TDM, which is based on the so-called\n",
        "    <span style=\"color:DarkOrange\">term frequency - inverse document\n",
        "    frequency</span>.\n",
        "\n",
        "-   Each $n_{ij}$ is the number of times the $j$th word appears in the\n",
        "    $i$th review, for $i = 1,\\dots,N$ and $j = 1,\\dots,p$.\n",
        "\n",
        "-   Let us define the following quantity:\n",
        "\n",
        "$$\n",
        "N_j = \\sum_{i=1}^N I(n_{ij} > 0) =  \\text{(\"Number of documents containing the j-th word\")}.\n",
        "$$\n",
        "\n",
        "-   Moreover, we define:\n",
        "\n",
        "$$\n",
        "n_{i \\cdot} = \\sum_{j=1}^p n_{ij} = \\text{(\"Number of words in the i-th document\")}.\n",
        "$$\n",
        "\n",
        "## TF-IDF transformation II\n",
        "\n",
        "-   The so-called <span style=\"color:DarkOrange\">term frequencies</span>\n",
        "    (TF) are just the fraction of times $j$th word appears in the $i$th\n",
        "    document, that is:\n",
        "\n",
        "$$\n",
        "f_{ij} = \\frac{n_{ij}}{n_{i \\cdot}}.\n",
        "$$\n",
        "\n",
        "-   The <span style=\"color:DarkOrange\">inverse document frequency</span>\n",
        "    (IDF) is a measure of how much information the word provides, that\n",
        "    is if it is common or rare across all documents. It is defined as\n",
        "    follows:\n",
        "\n",
        "$$\n",
        "\\text{IDF}_{j} = \\log\\left({\\frac{N}{N_j}}\\right).\n",
        "$$\n",
        "\n",
        "## TF-IDF transformation III\n",
        "\n",
        "-   The <span style=\"color:DarkOrange\">term frequency-inverse document\n",
        "    frequency</span> is then defined as:\n",
        "\n",
        "$$\n",
        "\\text{TF-IDF}_{ij} = f_{ij} \\times \\text{IDF}_j.\n",
        "$$\n",
        "\n",
        "-   In other words, the TF-IDF is just a weighted version of the\n",
        "    original frequencies $n_{ij}$, accounting for the fact that certain\n",
        "    words are “more relevant” (i.e., rare) than others."
      ],
      "id": "4e605b68-2a61-4811-8538-852073ba36fb"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Creation of a TDM TF-IDF with p = 500 words\n",
        "vectorizer = TfidfVectorizer(max_features = 500)\n",
        "X = vectorizer.fit_transform(imdb['review_token'])\n",
        "word_names = list(vectorizer.get_feature_names_out())\n",
        "\n",
        "# Conversion to dataframe\n",
        "X = pd.DataFrame(X.toarray())\n",
        "# Renaming columns according to words\n",
        "X.columns = word_names"
      ],
      "id": "2373a6f7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF-IDF transformation IV"
      ],
      "id": "224663a8-363d-4908-a542-ceeaf66b2fad"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/html": [
              "\n",
              "<p>8 rows × 500 columns</p>\n",
              "</div>"
            ]
          }
        }
      ],
      "source": [
        "X.head(8)"
      ],
      "id": "831affbf"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sentiment analysis I\n",
        "\n",
        "-   Sentiment analysis is the practice of understanding the overall\n",
        "    <span style=\"color:DarkOrange\">opinion</span> (sentiment) of a\n",
        "    document.\n",
        "\n",
        "-   It is arguably a very difficult (sometimes impossible) task,\n",
        "    especially in the presence of complex texts.\n",
        "\n",
        "-   Here, we showcase a straightforward algorithm for sentiment\n",
        "    analysis, based on the idea of\n",
        "    <span style=\"color:DarkOrange\">scoring</span>, called VADER.\n",
        "\n",
        "-   The [associated\n",
        "    article](https://ojs.aaai.org/index.php/ICWSM/article/view/14550/14399)\n",
        "    is:\n",
        "\n",
        "> Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based\n",
        "> Model for Sentiment Analysis of Social Media Text. Eighth\n",
        "> International Conference on Weblogs and Social Media (ICWSM-14). Ann\n",
        "> Arbor, MI, June 2014.\n",
        "\n",
        "## Sentiment analysis II\n",
        "\n",
        "-   The core idea is straightforward: “positive words” are given a\n",
        "    positive score, and vice versa with “negative words”.\n",
        "\n",
        "-   A human identifies whether these are positive or negative terms.\n",
        "\n",
        "-   Then, the scores are weighted, manipulated, and summarized through a\n",
        "    large number of <span style=\"color:DarkOrange\">heuristics</span>.\n",
        "\n",
        "-   Even though VADER is very simplistic, it is quick to compute and is\n",
        "    a reasonable starting point for more complex analysis. Let us see it\n",
        "    in action:"
      ],
      "id": "4fa65380-b5a6-4f71-9ac2-d976db4ba40f"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Please install the vader lexicon if it was not present\n",
        "# nltk.download('vader_lexicon')"
      ],
      "id": "7eff300b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sentiment analysis III\n",
        "\n",
        "-   Let us consider the 2nd review in our dataset, which is arguably\n",
        "    <span style=\"color:DarkOrange\">positive</span>."
      ],
      "id": "d56ba4fa-4aa1-40c3-a7ca-49a950b3a42d"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "'A wonderful little production. The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\\'s of comedy and his life. The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\\'s murals decorating every surface) are terribly well done.'"
            ]
          }
        }
      ],
      "source": [
        "review = basic_cleaning(imdb.iloc[1, 0])\n",
        "review"
      ],
      "id": "558fc002"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "{'neg': 0.055, 'neu': 0.768, 'pos': 0.177, 'compound': 0.9641}"
            ]
          }
        }
      ],
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "sentiment = SentimentIntensityAnalyzer()\n",
        "sentiment.polarity_scores(review)"
      ],
      "id": "b9111ac4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   The `compound` term is <span style=\"color:DarkOrange\">standardized\n",
        "    score</span> between $(-1, 1)$, measuring if sentiment is positive\n",
        "    or negative.\n",
        "\n",
        "-   In this case, the VADER algorithm correctly identifies the\n",
        "    sentiment.\n",
        "\n",
        "## Comparison with ChatGPT\n",
        "\n",
        "-   In some other cases, the VADER algorithm\n",
        "    <span style=\"color:DarkOrange\">fails badly</span>:"
      ],
      "id": "f5080255-e4de-43fe-9abf-310b44977ca0"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "\"This show was an amazing, fresh & innovative idea in the 70's when it first aired. The first 7 or 8 years were brilliant, but things dropped off after that. By 1990, the show was not really funny anymore, and it is continued its decline further to the complete waste of time it is today.It's truly disgraceful how far this show has fallen. The writing is painfully bad, the performances are almost as bad - if not for the mildly entertaining respite of the guest-hosts, this show probably wouldn't still be on the air. I find it so hard to believe that the same creator that hand-selected the original cast also chose the band of hacks that followed. How can one recognize such brilliance and then see fit to replace it with such mediocrity? I felt I must give 2 stars out of respect for the original cast that made this show such a huge success. As it is now, the show is just awful. I cannot believe it is still on the air.\""
            ]
          }
        }
      ],
      "source": [
        "review = basic_cleaning(imdb.iloc[7, 0])\n",
        "review"
      ],
      "id": "ecb84769"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Despite the quite negative review, the VADER algorithm produces the\n",
        "    following output:"
      ],
      "id": "df636b28-9019-43ad-98be-96e19808fd98"
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "{'neg': 0.149, 'neu': 0.654, 'pos': 0.197, 'compound': 0.8596}"
            ]
          }
        }
      ],
      "source": [
        "sentiment.polarity_scores(review)"
      ],
      "id": "03d73645"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is entirely inappropriate. Perhaps the words `amazing`, `fresh` and\n",
        "`innovative` (and others) misled the algorithm.\n",
        "\n",
        "## Comparison with ChatGPT\n",
        "\n",
        "-   [ChatGPT](https://openai.com/blog/chatgpt/) returns this:\n",
        "\n",
        "![](https://datasciencebocconi.github.io/Images/text_mining/chatbot.png)"
      ],
      "id": "9fbc3622-f530-46d2-b930-3b346044054c"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  }
}