{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EN2XGioRgoGK"
   },
   "source": [
    "<img src = \"https://datasciencebocconi.github.io/Images/Other/logoBocconi.png\">\n",
    "$\\newcommand{\\bb}{\\boldsymbol{\\beta}}$\n",
    "$\\DeclareMathOperator{\\Gau}{\\mathcal{N}}$\n",
    "$\\newcommand{\\bphi}{\\boldsymbol \\phi}$\n",
    "$\\newcommand{\\bpi}{\\boldsymbol \\pi}$\n",
    "$\\newcommand{\\bx}{\\boldsymbol{x}}$\n",
    "$\\newcommand{\\by}{\\boldsymbol{y}}$\n",
    "$\\newcommand{\\bmu}{\\boldsymbol{\\mu}}$\n",
    "$\\newcommand{\\bS}{\\boldsymbol{\\Sigma}}$\n",
    "$\\newcommand{\\whbb}{\\widehat{\\bb}}$\n",
    "$\\newcommand{\\hf}{\\hat{f}}$\n",
    "$\\newcommand{\\hy}{\\hat{y}}$\n",
    "$\\newcommand{\\tf}{\\tilde{f}}$\n",
    "$\\newcommand{\\ybar}{\\overline{y}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\Var}{Var}$\n",
    "$\\newcommand{\\Cov}{Cov}$\n",
    "$\\newcommand{\\Cor}{Cor}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GyZeMACgoGK"
   },
   "source": [
    "# Classification\n",
    "\n",
    "In some ways there is very little to say about classification: it is like regression but with categorical response. As before there will be a learning function $f(\\bx)$ but now the model that relates it to the response has to change to reflect the different characteristics thereof. But we can use the same set of tools - loss functions will change and as a result our tools of measuring performance too. \n",
    "\n",
    "On the other hand, this is a very common prediction problem and it is worth to understand deeper some of its intricacies\n",
    "\n",
    "We will start with binary classification - the response is one of two categories. The labelling of the categories is arbitrary and any sensible methodology should not rely on how these categories are coded numerically. We will stick to 0/1 coding for the two categories. This is mathematically more convenient for the methods we will use here. For other approaches to classification -1/1 might be more convenient. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2E-Xm2ZgoGK"
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this module we build predictive models for categorical outputs, following the same paradigm as in regression but changing the distribution that relates the output $y$ to the learning function $f(\\bx)$. We also adapt appropriately the model performance criteria and introduce concepts such as the misclassification probability, the ROC curve and AUC score. We discuss the problem of class imbalance and some solutions. We also contrast regression with Bayes classifiers. We show how to predict multicategorical and ordinal output in a simple framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5W9swZZigoGK"
   },
   "source": [
    "## The spam dataset\n",
    "\n",
    "This is a classic dataset for binary classification, it can be found in the UCI repository\n",
    "\n",
    "http://www.ics.uci.edu/~mlearn/MLRepository.html\n",
    "\n",
    "and specifically here:\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/spambase\n",
    "\n",
    "and it is analyzed in few different ways in the Hastie et al. book \n",
    "\n",
    "I have created a version with a subset of the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1095,
     "status": "ok",
     "timestamp": 1645032682937,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "RtZgkZfOgoGK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "# This is a Python module that contains plotting commands\n",
    "import matplotlib.pyplot as plt\n",
    "# the following provides further tools for plotting with dfs\n",
    "import seaborn as sns \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 339,
     "status": "ok",
     "timestamp": 1645032714991,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "s480hnGmdvme"
   },
   "outputs": [],
   "source": [
    "#import auxiliary functions (plot_confusion_matrix, get_auc)\n",
    "\n",
    "import requests\n",
    "url = \"https://datasciencebocconi.github.io/Code/helper_functions.py\"\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "open('downloaded_script.py', 'wb').write(r.content)\n",
    "from downloaded_script import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1645032715746,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "DMybtsicgoGK",
    "outputId": "7fceae12-5fb0-475e-ce31-c75820c0bb53"
   },
   "outputs": [],
   "source": [
    "spam = pd.read_csv(\"https://datasciencebocconi.github.io/Data/spam_small_train.csv\")\n",
    "spam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall: always check the documentation to understand what the data is about. From the UCI:\n",
    "\n",
    "\"48 continuous real [0,100] attributes of type word_freq_WORD\n",
    "= percentage of words in the e-mail that match WORD, i.e. 100 * (number of times the WORD appears in the e-mail) / total number of words in e-mail. A \"word\" in this case is any string of alphanumeric characters bounded by non-alphanumeric characters or end-of-string.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "executionInfo": {
     "elapsed": 354,
     "status": "ok",
     "timestamp": 1645032718576,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "lmBJ0B5PgoGM",
    "outputId": "46297ba5-d444-4453-bfeb-6512398d609e"
   },
   "outputs": [],
   "source": [
    "# Lets explore some basic aspects of this dataset\n",
    "\n",
    "spam[\"class\"].value_counts().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "executionInfo": {
     "elapsed": 1062,
     "status": "ok",
     "timestamp": 1645032830263,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "pl-0Gv_fgoGM",
    "outputId": "7529684b-f3e0-4d06-996c-a1ae7e9153aa"
   },
   "outputs": [],
   "source": [
    "spam.groupby(\"class\").boxplot(rot=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daJ_2GSIgoGN"
   },
   "source": [
    "The previous exploratory analysis is analogous to screening by correlations in regression. We could for example test nonparametrically equality of the two distributions - although these are only assessing marginal dependence and not interactions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qp1QWS61goGN"
   },
   "source": [
    "## Probabilistic classification using logistic regression\n",
    "\n",
    "We will follow a very analogous route to regression. There will be a learning function and we will take to be linear in parameters and features: \n",
    "\n",
    "$$ f(\\bx_i,\\bb) = \\bb^T \\bphi_i$$\n",
    "\n",
    "We will also model the distribution of the response and relate this distribution to the linear predictor\n",
    "\n",
    "There is no assumptions in this case for the distribution of the response: there is only one distribution to describe binary variables, the *Bernoulli*. Our probabilistic binary classification models take: \n",
    "\n",
    "$$ y_i \\sim Bernoulli(\\pi_i) \\iff p(y_i=1) = \\pi_i \\quad p(y_i=0) = 1-\\pi_i$$\n",
    "\n",
    "The remaining assumption we will make is how to relate $\\pi_i$ to the linear predictor. We basically need a transformation that maps real values to $[0,1]$. The *logistic transformation* is one option and leads to the so-called **logistic regression**:\n",
    "\n",
    "\\begin{equation}\n",
    "y_i \\sim Bernoulli\\left({1 \\over 1 + e^{-f(\\bx_i,\\bb)}}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "The negative sign is just for interpretation: large values of $f(\\bx_i,\\bb)$ are more likely associated to $y_i=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 705,
     "status": "ok",
     "timestamp": 1645032836197,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "6XpT4e33dvmh",
    "outputId": "1f025ffe-2a83-45e7-e068-a1b9f70f1085"
   },
   "outputs": [],
   "source": [
    "x = np.arange(-6,6,0.01)\n",
    "y = 1/(1+np.exp(-x))\n",
    "plt.plot(x,y)\n",
    "plt.axvline(x=0.0,color='r', linestyle='--')\n",
    "plt.axhline(y=0.5,color='r', linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-lo5P-2dvmi"
   },
   "source": [
    "\n",
    "Basic math shows that in this model \n",
    "\n",
    "$$- \\log p(y_i | \\bx_i) =  \\log (1+e^{-f(\\bx_i,\\bb)}) - y_i f(\\bx_i,\\bb) $$ \n",
    "\n",
    "hence, the loss function becomes \n",
    "\n",
    "$$L(\\bb) = \\sum_i \\log (1+e^{-f(\\bx_i,\\bb)}) - y_i f(\\bx_i,\\bb) $$\n",
    "\n",
    "This is also **convex** and can be optimized efficiently (this true for other *link* functions too, e.g. probit). A standard way to do this is using Fisher scoring, a variation of Newton-Raphson - these are **gradient-descent iterative optimization algorithms**. These work in the same way for the whole family of **generalized linear models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-sqzH54goGN"
   },
   "source": [
    "We now fit the model to the spam dataset. For the moment we will use the original variables as features\n",
    "\n",
    "Here we focus on predictive modelling and the `LogisticRegression` is reasonable. For inference this is not providing enough detail. This function actually optimizes over a penalized likelihood loss function, the default being a rigde penalty. We will set the regularization parameter to a very small value to keep its effect minimal for the time being ($n$ here is very large relative to $p$)\n",
    "\n",
    "Note that unlike `LinearRegression` now we can feed the function with dataframes and series... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1645032839919,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "x718s6EggoGN"
   },
   "outputs": [],
   "source": [
    "F = spam.drop('class', axis = 1)\n",
    "y = spam[\"class\"]\n",
    "\n",
    "# to print stats\n",
    "feature_names = F.columns\n",
    "class_labels = [\"email\",\"spam\"] # meant to represent 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 293,
     "status": "ok",
     "timestamp": 1645032872094,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "76JcrcNngoGN",
    "outputId": "da3c6dc7-86e3-4ba4-b4c7-f1dbd3abed6d"
   },
   "outputs": [],
   "source": [
    "# importing the relevant sklearn tools\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lregr = LogisticRegression(penalty='l2', C=100.0, fit_intercept=True, \n",
    "                           intercept_scaling=1, solver='liblinear', max_iter=500)\n",
    "\n",
    "# Fiting logistic regression\n",
    "\n",
    "lregr.fit(F,y)      \n",
    "\n",
    "# Compute the predicted probabilities in-sample\n",
    "\n",
    "insample_pred = lregr.predict_proba(F)\n",
    "\n",
    "insample_pred_res = spam.copy()\n",
    "\n",
    "insample_pred_res[\"pi_i\"] = insample_pred[:,1]\n",
    "\n",
    "print(insample_pred_res.iloc[1:15,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "\n",
    "This is a visualization exercise but very helpful for understanding the output of the model. Create a figure that contains the boxplots of predictive probabilities for each of the two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "executionInfo": {
     "elapsed": 387,
     "status": "ok",
     "timestamp": 1645032874868,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "Po69QohWgoGN",
    "outputId": "390c5a22-a2f5-447a-91ee-81c133fe86f6"
   },
   "outputs": [],
   "source": [
    "#Your code here \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CMbS57lHgoGN"
   },
   "source": [
    "## Turning probabilities into class prediction\n",
    "\n",
    "The *probabilistic classifier* we use explicitly accounts for misclassification errors. The algorithm returns probabilities and these also reflect the classification uncertainty\n",
    "\n",
    "Recall that if $y \\sim Bernoulli(\\pi)$ then $\\Var(y) = \\pi (1- \\pi)$, which is large for $\\pi \\approx 1/2$\n",
    "\n",
    "This is often the most useful type of classification output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCQXcF7-goGN"
   },
   "source": [
    "It might also be desirable to turn class probabilities into class prediction, e.g., to report medical tests (pregnant/no pregnant) - the spam example is also a good example: a decision has to be taken for each email and it is convenient to have the algorithm produce class predictions. We will denote those by $\\hy_i$\n",
    "\n",
    "With class prediction there will be **misclassification errors**: false positives and false negatives\n",
    "\n",
    "Before we go into details, let's see what the default operations in `sklearn` do for us. We can compute the following quantities in or out of sample and cross-validated too - basically all the discussion about use of sample for evaluation for regression applies here too. For the moment we experiment with in-sample calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1645032876608,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "TrWOxYQsgoGN",
    "outputId": "72876830-dcc5-4f96-ea66-a1787b92d587"
   },
   "outputs": [],
   "source": [
    "# this is the predict method in the LogisticRegression object\n",
    "y_hat = lregr.predict(F)\n",
    "y_hat[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "executionInfo": {
     "elapsed": 364,
     "status": "ok",
     "timestamp": 1645032878081,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "CvqOBUcPgoGN",
    "outputId": "d91bf8ac-8201-4b5e-b969-00cc87657353"
   },
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "cm =  confusion_matrix(y_pred=y_hat, y_true=y, labels=[0,1])\n",
    "print (cm)\n",
    "# Plotting confusion matrix (custom help function)\n",
    "\n",
    "plot_confusion_matrix(cm, class_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fy7nGKbgoGN"
   },
   "source": [
    "What `.predict` has done is it uses a threshold $c$ and if $\\pi_i \\geq c$ it sets $\\hy_i = 1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTk_4Eqddvmm"
   },
   "source": [
    "#### Exercise 2\n",
    "\n",
    "Write a code that takes as input the class probabilities returned by method *predict_proba* and a threshold $c$, returns class predictions 1 if those are larger than $c$ and 0 otherwise. Use a mix of experimentation and your intuition and identify what is the value of $c$ `.predict` uses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "executionInfo": {
     "elapsed": 850,
     "status": "ok",
     "timestamp": 1645032879940,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "tZFwYrVNgoGN",
    "outputId": "d2d45fd5-38b7-4f61-93b0-536fa5723e71"
   },
   "outputs": [],
   "source": [
    "#Your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-AmKnJdgoGN"
   },
   "source": [
    "Depending on the application the losses can be very different (remember the medical example). Let \n",
    "\n",
    "$$L(true,predict)$$ \n",
    "\n",
    "be a loss function that computes the cost of misclassification - this is something that requires context information not in the data. We take \n",
    "\n",
    "$$L(0,0) = L(1,1) = 0 \\quad L(1,0) > 0 \\quad L(0,1) > 0$$\n",
    "\n",
    "Little math shows that if $L(1,0)/L(0,1) = C$ the optimal decision is: \n",
    "\n",
    "$$\\hy_i = 1 \\iff \\pi_i > {1 \\over 1+C}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17firfURgoGN"
   },
   "source": [
    "### Misclassification rate\n",
    "\n",
    "$$p[ y \\neq \\hy]$$ \n",
    "\n",
    "is known as the misclassification probability. This can be estimated from data very much the same way as $R^2$: in sample (which can be negatively biased), out of sample (which is data intensive), by cross-validation (which is computationally intensive) etc; also by using more advanced math, such as *concentration inequalities*\n",
    "\n",
    "This number in isolation means pretty much nothing. Consider for example the (all too common situation) where we wish to predict $y$ in a population such that $p[y=1] = 0.001$. Then classifying everyone as $\\hy = 0$ yields a misclassification probability 0.001 but the algorithm is never able to identify the class of interest. Missclassification rate can be useful in comparisons. \n",
    "\n",
    "There are other performance metrics - related to conditional probabilities - e.g. $p[\\hy =1 | y=1]$ etc - such as specificity/sensitivity for qualifying the performance of a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDIxVt2RgoGN"
   },
   "source": [
    "### ROC curve and AUC\n",
    "\n",
    "A common tool to assess the performance of a probabilistic classifier is the ROC curve. Each point on this curve is (an estimate of) pair\n",
    "\n",
    "$$\n",
    "(p([\\hy = 1 | y = 0] , p[\\hy =1 | y = 1]) \n",
    "$$\n",
    "\n",
    "For a given threshold $c$ we can estimate these probabilities from the confusion matrix - obtained in the best way we can - simply by computing the associated frequencies\n",
    "\n",
    "As we vary the threshold the confusion matrices change and the frequencies too: varying $c$ from 0 to 1 we obtain the ROC curve - read the figure from right to left\n",
    "\n",
    "We do this now using a customized function. I will do it on a test spam data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "executionInfo": {
     "elapsed": 1287,
     "status": "ok",
     "timestamp": 1645032884818,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "hkKvwAZFgoGN",
    "outputId": "3941291c-8449-4747-c0a7-784d3bb57648"
   },
   "outputs": [],
   "source": [
    "# read the test data, extract the info and create predictions\n",
    "\n",
    "spam_test = pd.read_csv(\"https://datasciencebocconi.github.io/Data/spam_small_test.csv\")\n",
    "Ftest = spam_test.drop(\"class\",axis=1)\n",
    "ytest = spam_test[\"class\"]\n",
    "\n",
    "test_pred = lregr.predict_proba(Ftest)\n",
    "\n",
    "# Custom plot function\n",
    "get_auc(ytest, test_pred, class_labels, column=1, plot=True) # Helper function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_csXVj8CgoGN"
   },
   "source": [
    "AUC is the Area Under (the ROC) Curve\n",
    "\n",
    "It has though an interesting and solid statistical interpretation. It can be directly related to a non-parametric test - the **Mann-Whitney** that the following two samples come from the same distribution: \n",
    "\n",
    "$$\n",
    "\\textrm{class 1 probs }: \\{\\pi_i: y_i = 1\\} \\quad \\textrm{class 0 probs }: \\{\\pi_i: y_i = 0\\}\n",
    "$$\n",
    "\n",
    "You would expect that for a decent classifier the two samples come from different distributions and the distribution of the \"class 1 probs\" is stochastically greater. If the two distributions were identical we would obtain the green-dashed ROC curve.\n",
    "\n",
    "Consider a contest between the two samples:  each element of the first we compare with all of the elements in the second and record how many times it was at least as big. AUC is the frequency of won contests!\n",
    "\n",
    "For some `sklearn` tools check \n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zK62bxogoGN"
   },
   "source": [
    "#### Exercise 3\n",
    "\n",
    "As a sanity check, draw the curve for estimates using in-sample predictions.\n",
    "\n",
    "What do expect in-sample AUC to be, higher or lower than out-of-sample AUC? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "executionInfo": {
     "elapsed": 1233,
     "status": "ok",
     "timestamp": 1645032888640,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "PMKu7jQNgoGN",
    "outputId": "d7caa7e0-8bbf-4ecc-c88d-1a1971ef5ba7"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "Cuj9W5PXgoGO"
   },
   "source": [
    "## Class imbalance\n",
    "\n",
    "### Context\n",
    "\n",
    "Typically, we want to predict the incidence of a rare event:  rare disease, an accident, a default, exceptional performance, etc, on the basis of measured characteristics. \n",
    "\n",
    "If we collect data *prospectively* we will end up with a sample with very little representation of the class we are particularly interested in. Hence it will be hard to learn the function that separates \"rare\" from \"common\" on the basis of the measured characteristics; indeed classifiers will not mind overpredicting the \"common\". \n",
    "\n",
    "Lets look at an example first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOYU9BJHgoGO"
   },
   "source": [
    "### Can we predict good wine ? \n",
    "\n",
    "This is another standard dataset available from the UCI repository. It is really about **ordinal regression** - or maybe **multiclass classification**. Here we will for illustration consider (at the expense of losing information) a transformation of the response \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1645032890296,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "7cW_PxFlgoGO"
   },
   "outputs": [],
   "source": [
    "wine_df = pd.read_csv(\"https://datasciencebocconi.github.io/Data/wine.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 391,
     "status": "ok",
     "timestamp": 1645032891446,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "QTKmpCnJgoGO",
    "outputId": "a4f17ab9-60b8-4d50-86e5-dd7577b01b98"
   },
   "outputs": [],
   "source": [
    "wine_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 228,
     "status": "ok",
     "timestamp": 1645032892462,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "kSMOaVeagoGO",
    "outputId": "1eb21165-ad95-4d2b-c8a7-2fdc01d25af4"
   },
   "outputs": [],
   "source": [
    "wine_df.quality.value_counts(sort=False).plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "\n",
    "Write a code to create a new culumn in the dataframe with name \"bin_quality\" and takes the value 1 if quality is bigger or equal to 8 and 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1645032894640,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "O9TM9OvQgoGO"
   },
   "outputs": [],
   "source": [
    "#Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nnmohmYEgoGO"
   },
   "source": [
    "This is class imbalance for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1645032897901,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "5GriGq3ggoGO"
   },
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "X = wine_df.drop(['quality',\"bin_quality\"], axis =1)\n",
    "y = wine_df.bin_quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1645032898796,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "4O8haDMmgoGO"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# I should have standardised features but here I am using tiny regularisation so it should not matter\n",
    "model = LogisticRegression(C=100, solver='liblinear') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "executionInfo": {
     "elapsed": 1105,
     "status": "ok",
     "timestamp": 1645032901458,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "Lg0fjx9wgoGO",
    "outputId": "651616f7-cb87-4dfa-f3e9-4af9005b9dd7"
   },
   "outputs": [],
   "source": [
    "# AUC\n",
    "y_probabilities = cross_val_predict(model, X, y, method='predict_proba', cv = 5)\n",
    "get_auc(y, y_probabilities, [\"Bad/Average Wine\", \"Great Wine\"], column=1, plot=True) # Help function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 234,
     "status": "ok",
     "timestamp": 1645032904406,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "vgclpykUgoGO",
    "outputId": "ff0a5c97-b951-4e6d-da7d-23a11341d50d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = cross_val_predict(model, X, y, method='predict', cv = 5)\n",
    "print (\"Accuracy (cross-validated): \", accuracy_score(y, y_pred))\n",
    "\n",
    "####  Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print (classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8a32InkgoGO"
   },
   "source": [
    "**The operation was successful but the patient died!**\n",
    "\n",
    "As the following exercise demonstrates, none of the good wines are classifed as such"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZ5-i8-MgoGO"
   },
   "source": [
    "#### Exercise 5\n",
    "\n",
    "Plot the confustion matrix to verify results. Are we predicting the 'good wines'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "executionInfo": {
     "elapsed": 765,
     "status": "ok",
     "timestamp": 1645032911208,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "77N8E9rlgoGO",
    "outputId": "d9a7d65a-c224-492e-a07a-640e724fc4e2"
   },
   "outputs": [],
   "source": [
    "# Your code here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJCFnHG6goGO"
   },
   "source": [
    "## Some approaches to class imbalance\n",
    "\n",
    "Roughly speaking there are two ways to try and do better: \n",
    "\n",
    "+ Retrospective study & bias correction: this operates at the *design* stage, that is the way data are collected in the first place \n",
    "\n",
    "+ Resampling and use of synthetic data (& bias correction): this works with the data at hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ozgt7q1ogoGO"
   },
   "source": [
    "### Retrospective studies\n",
    "\n",
    "Collect data not as a *representative* sample from the population of interest, but oversample the rare class; for example from your medical database choose a sample of $n/2$ patients with the rare disease and $n/2$ without\n",
    "\n",
    "On the basis of this *biased (non-representative)* sample train a probabilistic classifier, e.g., logistic regression \n",
    "\n",
    "The estimated learning function is biased too - it will predict way larger probability of class=1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnVfDwijgoGO"
   },
   "source": [
    "There are at least two ways to do proper inference with the non-representative sample. \n",
    "\n",
    "Lets say that $q(y)$ are the probabilities of the two classes in the population of interest, i.e., $q(1), q(0)$. And $r(y)$ are the probabilities with which we have sampled \n",
    "\n",
    "+ One is to change the loss function: instead of using the log-likelihood, which is an *arithmetic average* of individual log-densities, use a *weighted average*\n",
    "\n",
    "  Some math shows that the following is a valid choice: \n",
    "  \n",
    "  $$ L(\\bb) = \\sum_i {q(y_i) \\over r(y_i)} \\left [\\log (1+e^{-f(\\bx_i,\\bb)}) - y_i f(\\bx_i,\\bb)\\right ] $$\n",
    "  \n",
    "+ Another is to run the analysis with the biased sample and the log-likelihood loss, but then rescale the estimated probabilities\n",
    "\n",
    "  Some math shows that if $\\pi(y_i)$ are the probabilities estimated by the model, they should be changed to \n",
    "  \n",
    "  $$ { \\pi(y_i) {q(y_i) \\over r(y_i)} \\over \\pi(1) {q(1) \\over r(1)} + \\pi(0) {q(0) \\over r(0)}}$$\n",
    "  \n",
    "  Lets understand what is the effect of this weighting: consider very small and very large predicted $\\pi_i$\n",
    "  \n",
    "Both approaches require that $q(y)$ is known - but this is often easy enough "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8i0zfqAugoGO"
   },
   "source": [
    "### Resampling approaches\n",
    "\n",
    "Having no control of the data collection protocol, one can try and sharpen the distinction between the two classes by either under-representing (*undersampling*) the popular class, or over-representing (*oversampling*) the rare class, or both\n",
    "\n",
    "Oversampling might just be randomly replicating rare cases or creating synthetic rare cases that \"look like\" the rare cases in the sample. To this respect doing some modelling on the $\\bx_i$s can help - and there are links to *Bayes classifiers* we mention later and to so-called *generative models*. For example SMOTE (Synthetic Minority Oversampling Technique) does this\n",
    "\n",
    "Any of these approaches needs to be combined with one of the bias-correction approaches developed above\n",
    "\n",
    "Some tools in `sklearn` to do this are, e.g., the module `imblearn` and its methods `.over_sampling`, e.g. `RandomOverSampler` and `SMOTE`. `imblearn` requires installing - do not do this now! Analogous result to `.over_sampling` is obtained using `LogisticRegression`  `class_weight=\"balanced\"` argument, that corresponds to oversampling\n",
    "\n",
    "In our wine prediction example we have also done some arbitrary dichotomization of the response: no good reason why we should not pay for it!!\n",
    "\n",
    "Lets try this one for the wine data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1645032915944,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "qpCvtUu9goGP"
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=100, solver='liblinear')\n",
    "model.fit(X,y)\n",
    "y_prob = model.predict_proba(X)\n",
    "model = LogisticRegression(C=100, class_weight='balanced', solver='liblinear')\n",
    "model.fit(X,y)\n",
    "y_prob_imb = model.predict_proba(X)\n",
    "wine_df[\"pred_prob\"] = y_prob[:,1] \n",
    "wine_df[\"pred_prob_imb\"] = y_prob_imb[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8v6OOlIHgoGP"
   },
   "source": [
    "Note that these are not proper estimates of the class probability: we need to correct for the biased sample by rescaling the predicted probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1645032917234,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "MaqQc7BIgoGP"
   },
   "outputs": [],
   "source": [
    "## the correction factor: \n",
    "\n",
    "q1 = y.sum()/len(y)\n",
    "r1 = 0.5\n",
    "\n",
    "def reweight(pi,q1=0.5,r1=0.5):\n",
    "    r0 = 1-r1\n",
    "    q0 = 1-q1\n",
    "    tot = pi*(q1/r1)+(1-pi)*(q0/r0)\n",
    "    w = pi*(q1/r1)\n",
    "    w /= tot\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 304,
     "status": "ok",
     "timestamp": 1645032919756,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "xpHbCZKugoGP"
   },
   "outputs": [],
   "source": [
    "# correcting for biased sample\n",
    "\n",
    "wine_df[\"pred_prob_imb_corr\"] = wine_df[\"pred_prob_imb\"].apply(reweight,args=(q1,r1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "executionInfo": {
     "elapsed": 1185,
     "status": "ok",
     "timestamp": 1645032921766,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "BoSndj2NgoGP",
    "outputId": "0ed650dd-12f6-472d-89c7-9fd1eb2902a0"
   },
   "outputs": [],
   "source": [
    " \n",
    "wine_df.boxplot([\"pred_prob\",\"pred_prob_imb\",\"pred_prob_imb_corr\"],by=\"quality\",layout=(1,3), figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "executionInfo": {
     "elapsed": 461,
     "status": "ok",
     "timestamp": 1645032924169,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "iQ0l_G_SgoGP",
    "outputId": "a51497ae-2e27-4909-9b74-f9f9a0e49664"
   },
   "outputs": [],
   "source": [
    "y_pred_new = [1 if pi >= 0.5 else 0 for pi in wine_df[\"pred_prob_imb_corr\"] ]\n",
    "\n",
    "#confusion matrix\n",
    "cm =  confusion_matrix(y_pred=y_pred_new, y_true=y, labels=[0,1])\n",
    "# Plotting confusion matrix (custom help function)\n",
    "plot_confusion_matrix(cm, [\"Bad/Average Wine\", \"Great Wine\"]) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KUfuXYjGgoGP"
   },
   "source": [
    "## Regularization\n",
    "\n",
    "Everything that applies to regression applies to logistic regression: ridge, lasso etc penalties can be used for high-dimensional feature spaces and when they are convex the resultant loss function is too and the algorithms are efficient \n",
    "\n",
    "Indeed, the `LogisticRegression` function by default includes the penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZuxauX1goGP"
   },
   "source": [
    "## Multiclass classification\n",
    "\n",
    "Often the output variable is categorical with many options, not just two, e.g. the wine dataset. \n",
    "\n",
    "It is more convenient to encode such multiclass output using the 1-hot encoding, i.e., each output is a vector of 0s with a single 1 in the chosen class: \n",
    "\n",
    "$$\\by_i = (y_{i1},\\ldots,y_{iK})^T \\quad y_{ij} \\in \\{0,1\\}, \\quad \\sum_j y_{ij} = 1$$\n",
    "\n",
    "\n",
    "where the labels $1,2,\\ldots,K$ are arbitrary encodings for the different output categories and any sensible analysis should not depends on their values. \n",
    "\n",
    "Binary classification takes $K=2$. In fact, in modern applications $K \\sim 100$ or even $K \\sim 1000$ (e.g. *recommendation*).\n",
    "\n",
    "The most direct extension of the binary regression to multiclass is as follows. We take\n",
    "\n",
    "$$\\by_i \\sim Categorical(\\pi_{i1},\\ldots,\\pi_{iK})$$\n",
    "\n",
    "with density \n",
    "\n",
    "$$p(\\by_i) = \\prod_{j} \\pi_{ij}^{y_{ij}}$$\n",
    "\n",
    "which is a clever way to simply say that the probability that the $j$th category is chosen is $\\pi_{ij}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDL7rg5UgoGP"
   },
   "source": [
    "### Multinomial-logistic regression\n",
    "\n",
    "We need to map the probabilities $\\pi_{ij}$ to the input $\\bx_i$. One way that collapses to logistic regression when $K=2$ is to take:\n",
    "\n",
    "$$\\log{\\pi_{ij} \\over \\pi_{i1}} = f(\\bx_i,\\bb_j)$$\n",
    "\n",
    "Note that implicit in this definition is that the odds to choose $j$ vs 1 do not depend on what other options there exist: this is known as the *independence of irrelevant alternatives* assumptions and is criticized in certain contexts. \n",
    "\n",
    "The model definition implies\n",
    "\n",
    "$$\\pi_{ij} = {e^{f(\\bx_i,\\bb_j)} \\over 1+ \\sum_{k>1} e^{f(\\bx_i,\\bb_k)}}$$\n",
    "\n",
    "and you should check that for $K=2$ this is precisely logistic regression. The pivot category is taken above to be 1, but any other can be chosen - this only affects the interpretation of the results. Note also that we have different parameters $\\bb_j$ for each category $j$. \n",
    "\n",
    "This model is known by a multitude of names...\n",
    "\n",
    "https://en.wikipedia.org/wiki/Multinomial_logistic_regression\n",
    "\n",
    "The negative log-likelihood is immediatelly obtained and is **convex** in the $\\bb_j$s, hence we have a nice learning problem to solve. In fact, an old clever trick can be used to turn learning this model into a Poisson GLM, this is known as the *Poisson trick* in the Stats community. This is particularly important for large $K$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3eaWU6NkgoGP"
   },
   "source": [
    "### Reanalyzing the wine data\n",
    "\n",
    "`LogisticRegression` in `sklearn` does in fact also fit the multinomial-logistic regression model. Lets try this out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1568,
     "status": "ok",
     "timestamp": 1645032929552,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "370e5GbtgoGP",
    "outputId": "ee2abbb7-bf19-4e73-9222-28eed0928702"
   },
   "outputs": [],
   "source": [
    "# multinomial-logistic regression for wine\n",
    "model = LogisticRegression(C=100,multi_class=\"multinomial\",solver=\"newton-cg\",max_iter=10000) \n",
    "X = wine_df.drop(['quality',\"bin_quality\"], axis =1)\n",
    "y = wine_df.quality\n",
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5472,
     "status": "ok",
     "timestamp": 1645032936078,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "iIFxNNQLgoGP"
   },
   "outputs": [],
   "source": [
    "y_probabilities = cross_val_predict(model, X, y, method='predict_proba', cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "executionInfo": {
     "elapsed": 6031,
     "status": "ok",
     "timestamp": 1645032943787,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "dy9HzrmlgoGP",
    "outputId": "3d770e05-1974-48c3-d842-b6ecbbcf60d8"
   },
   "outputs": [],
   "source": [
    "y_pred = cross_val_predict(model, X, y, cv = 5)\n",
    "cm =  confusion_matrix(y_pred=y_pred, y_true=y, labels=[3,4,5,6,7,8])\n",
    "# Plotting confusion matrix (custom help function)\n",
    "plot_confusion_matrix(cm, [\"3\",\"4\",\"5\",\"6\",\"7\",\"8\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZvWeH5RxgoGP"
   },
   "source": [
    "Notice the shrinkage towards the 5!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HeXRgM7dvmz"
   },
   "source": [
    "## Bayes classifiers\n",
    "\n",
    "An entirely different - but turns out to be related (hence included here) - approach to classification is to built a **joint model** for\n",
    "\n",
    "$$\n",
    "p(\\bx,y)\n",
    "$$\n",
    "\n",
    "as opposed for the conditional \n",
    "\n",
    "$$\n",
    "p(y | \\bx)\n",
    "$$\n",
    "\n",
    "that the previous approach we consider does. The fact that the joint model gives a recipe for generating data makes this approach be referred to as **generative**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiF8Frf7dvm0"
   },
   "source": [
    "Bayes classifiers come up with a joint model by decomposing the joint probabilities: \n",
    "\n",
    "$$p(\\bx,y) = p(y) p(\\bx | y)$$\n",
    "\n",
    "Focusing on binary classification, one learns \n",
    "\n",
    "1. $p(y=1)$ - this is trivial\n",
    "2. $p(\\bx | y=1)$ and $p(\\bx | y=0)$; the two conditional distributions\n",
    "\n",
    "With these, predictive probabilities for the class are obtained using the **Bayes theorem** (hence the name) \n",
    "\n",
    "$$p(y =1 | \\bx) = { p(\\bx | y=1) p(y =1) \\over p(\\bx | y=1) p(y =1)  + p(\\bx | y=0) p(y =0)} $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5U6kpxW0dvm0"
   },
   "source": [
    "The challenge is to come up with tractable and useful models for $p(\\bx |y)$ - non-trivial since we typically have 10ths/100ds/1000ds of features\n",
    "\n",
    "Two off-the-shelf options are: \n",
    "\n",
    "1. $\\bx | y = i \\sim \\Gau(\\bmu_i, \\bS)$, for $i=0,1$. The resultant classifier is known as **Fisher discriminant analysis**\n",
    "2. $p(\\bx | y = i)  = \\prod_{j=1}^p p_{i,j}(x_j)$ for $\\bx = (x_1,\\ldots,x_p)^T$; the resultant classifier is known as **naive Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWjqKiQvdvm0"
   },
   "source": [
    "It is well known, e.g. since Efron (1975, JASA), that discriminant analysis is equivalent to logistic regression with specific coefficients - the article shows that it is not that good idea to use the former\n",
    "\n",
    "Naive Bayes is not functionally related to logistic regression but theory exists about their relative performance. In a nutshell, naive Bayes classifiers reach near-optimal performance with smaller sample sizes but their optimal performance is worse than that of logistic regression\n",
    "\n",
    "Still, subject matter knowledge and more clever modelling on $p(\\bx|y)$ can improve the performance of Bayes classifiers\n",
    "\n",
    "Lets revisit an analysis we did with the spam dataset and appreciate the implicity Bayes classifier feel to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "executionInfo": {
     "elapsed": 1146,
     "status": "ok",
     "timestamp": 1645032952293,
     "user": {
      "displayName": "Tommaso Rigon",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhoZ5uPN0szloXuIfFrSIPrIdooEMh4LTdjPP22=s64",
      "userId": "08861669753495411183"
     },
     "user_tz": -60
    },
    "id": "UuGjt8Sbdvm0",
    "outputId": "442e23bb-2a64-4cab-af52-3d12bd474057"
   },
   "outputs": [],
   "source": [
    "spam.boxplot(column=[\"word_freq_you\",\"word_freq_hp\",\"char_freq_!\"], by = \"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uOtLshVgoGP"
   },
   "source": [
    "## Some hints for the practitioners\n",
    "\n",
    "+ Predictive modelling with categorical output can be done using `LogisticRegression`; by default this module includes regularization, hence it is straighforward to include hundreds of features\n",
    "+ When the output is multicategorical is way more sensible to build directly a model for the original output than first turn it (more or less arbitrarily) into a binary output. Even if for commercial/interpretability purposes a binary prediction is preferred, is preferrable to turn the multicategorical prediction into binary rather than the multicategorical output to binary and build a model\n",
    "+ A probabilistic classifier returns probabilities for the possible categories. It is not the data scientist's job to turn those into class predictions. This should be done in conjunction with the user of the analysis and the consideration of losses. Once the losses have been specified a simple formula gives the optimal conversion\n",
    "+ Class imbalance is an issue relevant for many or even most classification applications. Using the `class_weight=\"balanced\"` within `LogisticRegression` gives a possible improvement using oversampling - make sure to correct the probabilities it returns since they are not correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROBwbAUngoGP"
   },
   "source": [
    "## References\n",
    "\n",
    "Hastie, T., Tibshirani, R., Friedman, J., 2009. *Elements of Statistical Learning*. 2nd Edition. Section 4.4; More advanced 3.8,3.9.  https://web.stanford.edu/~hastie/ElemStatLearn/\n",
    "\n",
    "Bishop, C.M. *Pattern recognition and machine learning*. Sections 4.2, 4.3"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "17firfURgoGN",
    "WDL7rg5UgoGP"
   ],
   "name": "classification_tutorcopy.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
